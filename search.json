[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Location–scale models in ecology and evolution: handling heterogeneity in continuous, count and proportion data",
    "section": "",
    "text": "1 Preparation\nThis tutorial provides a comprehensive guide to location-scale models, focusing on their application in ecology and evolution. Location-scale models allow researchers to simultaneously model both the mean (location) and variability (scale) of a response variable as functions of predictors. This is useful in biological data analysis, where understanding not just the average response but also the variability around that average can provide deeper insights into biological processes.\nSome of the models in this tutorial take a long time to run. To make the tutorial faster and easier to follow, we have precomputed selected models and saved them as .rds files.\nIf you would like to re-run the code yourself and reproduce the results shown in the tutorial, please make sure to do the following:\nYou will see similar code snippets throughout the tutorial. Feel free to modify them based on your own setup. If everything is placed correctly, the .qmd file should run without any issues.\nThis tutorial provides a step-by-step guide to applying location-scale models in ecology, evolution, and environmental sciences. We focus on practical applications and demonstrate how to implement these models in R using the glmmTMB and brms packages.\nFor models that can be implemented in both (e.g., Model 1, Model 2 or the negative binomial model in Beyond Gaussian 1), we provide code for both packages. However, for models that are currently only feasible in brms package (e.g., Model 3: where the location and scale parts include random effects), we illustrate the analysis using it alone.\nAfter a brief introduction to both modeling frameworks, we present worked examples using real datasets. In the final section, we also discuss approaches for model comparison and selection.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#load-required-packages",
    "href": "index.html#load-required-packages",
    "title": "Location–scale models in ecology and evolution: handling heterogeneity in continuous, count and proportion data",
    "section": "\n1.1 Load required packages",
    "text": "1.1 Load required packages\nOur tutorial uses R statistical software and existing R packages, which you will first need to download and install.\nThis tutorial makes use of several R packages for data manipulation, model fitting, diagnostics, visualisation, and reporting:\n\ndplyr, tibble, tidyverse - for efficient and tidy data manipulation.\nbrms, glmmTMB, arm - for fitting generalised linear mixed models.\ncmdstanr - interfaces with the CmdStan backend to accelerate Bayesian sampling via within-chain parallelisation. is a C++ library for Bayesian inference, in order to enable within-chain parallelisation to speeding up the sampling process.\nDHARMa, loo, MuMIn - for model diagnostics, cross-validation, and multi-model inference (note that both loo and Mumin contain functions with the same names, so call them with explicit namespaces, e.g. loo::loo( )).\nggplot2, patchwork, bayesplot, tidybayes - for flexible and publication-ready visualisations.\ngt, kableExtra, knitr - for creating clean tables.\nhere - for consistent file path management across projects.\nape, TreeTools - for phylogenetic analyses and tree manipulation.\n\n\n# Load required packages\n\npacman::p_load(\n  ## data manipulation\n  dplyr, tibble, tidyverse, broom, broom.mixed,\n  \n  ## model fitting\n  ape, arm, brms, broom.mixed, cmdstanr, emmeans, glmmTMB, MASS, phytools, rstan, TreeTools,\n  \n  ## model checking and evaluation\n  DHARMa, loo, MuMIn, parallel,\n  \n  ## visualisation\n  bayesplot, ggplot2, patchwork, tidybayes,\n  \n  ## reporting and utilities\n  gt, here, kableExtra, knitr\n)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#glmmtmb-vs-brms",
    "href": "index.html#glmmtmb-vs-brms",
    "title": "Location–scale models in ecology and evolution: handling heterogeneity in continuous, count and proportion data",
    "section": "\n1.2 glmmTMB vs brms\n",
    "text": "1.2 glmmTMB vs brms\n\nglmmTMB is a powerful and flexible R package for fitting generalized linear mixed models (GLMMs), including models with random effect structures and scale (dispersion) part. It is built on the Template Model Builder (TMB) framework, which allows fast and efficient maximum likelihood estimation even for large and complex models.\nbrms is an R package that allows users to fit Bayesian generalized (non-)linear multilevel models using the probabilistic programming language Stan. It provides a user-friendly formula syntax similar to that of lme4 or glmmTMB, and supports a wide range of distributions, link functions, and advanced model components, including location-scale modeling.\nBoth packages are suitable for fitting location-scale models and are widely used in ecology and its related fields. Therefore, we selected them to illustrate how location-scale models can be practically applied in real data analysis.\nWhile brms is a powerful and flexible package for Bayesian regression modeling, some readers may not be familiar with its usage. Below, we provide a brief introduction to fitting models using brms, focusing on the basic location-scale structure and key functions relevant to our analysis. You can also find some examples each section…\nIf you get stuck or are unsure about something, it might be helpful to check the below:\n\nhttps://paulbuerkner.com/brms/index.html\nhttps://discourse.mc-stan.org/\n\nThis example shows how to fit a simple location-scale model, where both the mean (\\(\\mu\\)) and the variability (\\(\\sigma\\)) of a continuous outcome variable \\(y\\) are modeled as functions of a predictor \\(x\\). Note that, throughout this tutorial, we explicitly write y ~ 1 + x to indicate that the model includes an intercept. However, y ~ x also includes an intercept by default, so both formulations are equivalent.\n\n# Example dataset\n# y is continuous response, x is a predictor\n# specify the model using bf()\nformula1 &lt;- bf(\n  y ~ 1 + x,  # location part\n  sigma = ~ 1 + x # scale part - specified by sigma\n)\n\n# generate default priors based on the formula and data\ndefault_priors &lt;- default_prior(\n                        formula1,\n                        data = dat,                             \n                        family = gaussian()                               \n                          )\n\n# fit the model - you can change N of iter, warmup (and thin), and also chains.\n  m1 &lt;- brm(formula1,\n                  data = dat,           \n                  family = gaussian(),                   \n                  prior = default_priors,                \n                  iter = 2000,   # total iterations per Markov-chain (i.e., how many posterior samples are drawn, including warm-up)\n                  warmup = 1000, # number of early draws used only for adapting the sampler (step-size, mass matrix). These samples are discarded\n                  thin = 1,      # keep every n-th post-warm-up draw. 1 means keep all draws                    \n                  chains = 2,    #  number of independent MCMC chains run in parallel. Provides a convergence check (via Rhat)                    \n             )\nsummary(m1)\n\nAfter fitting the model, you can use summary(m1) to inspect the estimated coefficients and sigma with 95% Credible Intervals, along with diagnostic statistics such as Rhat and effective sample size. To better understand how to interpret the model output, please refer to the “Bonus - brms” part in the next section.\n\n1.2.1 Parallel Processing\nBefore fitting our models with brms, we configure some global options to optimize sampling speed using parallel processing:\n\n\nparallel::detectCores(): This function automatically detects the number of logical CPU cores available on your machine. This is a convenient way to ensure your code adapts to different computing environments.\n\noptions(mc.cores = parallel::detectCores()): The mc.cores option is a global setting primarily used by rstan (the engine behind brms). It controls the number of MCMC chains that will be run in parallel. By setting it to detectCores(), you are telling brms to run as many chains concurrently as your CPU allows, significantly speeding up the overall sampling process.\n\noptions(brms.threads = 6): The brms.threads option specifies the number of CPU threads that Stan’s internal operations can utilize within a single MCMC chain. This enables within-chain parallelisation, further accelerating computations, especially for complex or large datasets. The value 6 is an example; you can adjust this based on your specific CPU architecture and memory.\n\n\n\n\n\n\n\nThreading is a powerful feature that enables you to split a chain into multiple parallel threads, significantly reducing computation time. However, it requires installing both cmdstanr and the underlying CmdStan backend.\n\nCodecmdstanr::install_cmdstan()\ncmdstanr::check_cmdstan_toolchain()\ncmdstanr::cmdstan_version()\n\n\n\n\n\nThese settings are crucial for making Bayesian model fitting with brms more efficient, particularly for complex models or large datasets.\n\nparallel::detectCores()\n\n[1] 12\n\noptions(mc.cores = parallel::detectCores())\n\noptions(brms.threads = 6)  # Set global default\n\n\n\n\n\n\n\nbrms models can be computationally intensive and take a significant amount of time to run. To streamline your workflow, we provide the pre-fit models in RDS files, allowing you to load them directly without needing to re-run the lengthy estimation process.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#specifying-the-scale-component",
    "href": "index.html#specifying-the-scale-component",
    "title": "Location–scale models in ecology and evolution: handling heterogeneity in continuous, count and proportion data",
    "section": "\n1.3 Specifying the Scale Component",
    "text": "1.3 Specifying the Scale Component\nHere’s how the scale component is handled in glmmTMB and brms, along with common parameter names for various distributions:\n\n\n\n\n\n\n\n\nDistribution\nScale Parameter (Example)\n\nglmmTMB Specification\n\nbrms Specification (Example)\n\n\n\nGaussian\n\\(\\sigma\\)\ndispformula = ~ ...\nbf(..., sigma ~ ...)\n\n\nNegative Binomial\n\\(\\theta\\)\ndispformula = ~ ...\nbf(..., shape ~ ...)\n\n\nConway-Maxwell-Poisson\n\\(\\nu\\)\ndispformula = ~ ...\nbf(..., shape ~ ...)\n\n\nBeta-Binomial\n\\(\\phi\\)\ndispformula = ~ ...\nbf(..., phi ~ ...)\n\n\n\nNote: In glmmTMB, dispformula is generally used to model the dispersion or scale parameter, regardless of its specific Greek letter notation, which varies by distribution.\nThe scale part varies depending on the distribution: for example, \\(\\sigma\\) for Gaussian or \\(\\theta\\) for negative binomial, \\(\\nu\\) for Conway–Maxwell–Poisson, \\(\\phi\\) for beta-binomial distribution (see the main text).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#interpreting-regression-coefficients-on-the-log-scale",
    "href": "index.html#interpreting-regression-coefficients-on-the-log-scale",
    "title": "Location–scale models in ecology and evolution: handling heterogeneity in continuous, count and proportion data",
    "section": "\n1.4 Interpreting regression coefficients on the log scale",
    "text": "1.4 Interpreting regression coefficients on the log scale\nBefore moving on to the examples, we provide a brief overview of how to interpret regression coefficients when the response variable or a distributional parameter is modelled on the non-natural scale. This is relevant for many of the models we will discuss in this tutorial. The models we use (Gaussian with log-transformed response, negative binomial, zero-one-inflated beta, Conway–Maxwell–Poisson, etc.) employ a log link for at least one distributional parameter. In these cases, regression coefficients (\\(\\beta\\)) describe multiplicative changes in the parameter of interest. After exponentiation, these can usually be interpreted as percentage changes, with the important exception of logit-linked parameters, which yield odds ratios instead.\n\n1.4.1 Rule of thumb\n\n\nWhen using a log link:\n\n\n\\(\\exp(\\beta)\\) gives the multiplicative ratio associated with a one-unit increase in the predictor.\n\n\\(100 \\times (\\exp(\\beta) - 1)\\) gives the percent change associated with a one-unit increase (positive) / decrease (negative) in the predictor.\nThis applies to parameters such as the mean, variance/dispersion, or precision when they are modelled with a log link.\n\n\n\nWhen using a logit link:\nExponentiated coefficients represent odds ratios, not percentage changes.\nExample: \\(\\beta = 0.7 \\;\\Rightarrow\\; \\exp(0.7) \\approx 2.0\\), meaning the odds are about twice as high for a one-unit increase in the predictor.\n\n1.4.2 Intercept vs. coefficients\n\n\nIntercept:\n\nIf the response was log-transformed before fitting (e.g. Gaussian on log(Y)), exponentiating the intercept gives the mean of Y in the reference group.\n\nIf a log link was used (e.g. Poisson, negative binomial), exponentiating the intercept gives the mean of Y in the reference group.\n\n\n\nCoefficients:\n\nEach coefficient is an additive effect on the log scale. After exponentiation, it represents a multiplicative ratio on the original response scale.\n\nExample: \\(\\beta = 0.05 \\;\\Rightarrow\\; \\exp(0.05) \\approx 1.05\\) -&gt; the expected response is about 5% larger.\n\nNegative coefficients imply proportional decreases (e.g. \\(\\beta = -0.30 \\;\\Rightarrow\\; \\exp(-0.30) \\approx 0.74\\) -&gt; about a 26% reduction).\n\n\n\n1.4.3 What the change applies to…\n\n1.4.3.1 Location (mean of Y)\n\nLog-transformed response (e.g. Gaussian on log(Y)):\nCoefficients describe multiplicative changes in the original response Y.\nModels with a log link (e.g. Poisson, negative binomial, Gaussian with log link):\nCoefficients describe multiplicative changes in the mean of Y.\nZero-one-inflated beta (mean, logit link):\nExponentiated coefficients are odds ratios, not percent changes.\n\n1.4.3.2 Scale (variance, dispersion, precision)\n\nNegative binomial (log link for \\(\\theta\\), precision):\nCoefficients describe % changes in the precision parameter \\(\\theta\\) (larger \\(\\theta\\) = less overdispersion).\nConway–Maxwell–Poisson (\\(\\nu\\), log link):\nCoefficients describe % changes in \\(\\nu\\) (\\(\\nu &gt; 1\\) = under-dispersion, \\(\\nu &lt; 1\\) = over-dispersion).\nZero-one-inflated beta:\nPrecision \\(\\phi\\) (log link): interpret as % change in \\(\\phi\\) (larger \\(\\phi\\) = less extra-binomial scatter).\nZero-inflation (logit link): exponentiated coefficients are odds ratios for the probability of structural zeros or ones.\n\n1.4.4 Multiple-unit changes, interactions, and uncertainty\n\n\nInteractions: Sum the relevant coefficients (main effects + interaction term) before exponentiating.\n\nWhy not sum CIs directly? Because fixed effects are correlated. Proper CI calculation requires using the full variance–covariance matrix (frequentist) or summing posterior draws (Bayesian).\n\n\n\n\nConfidence/credible intervals: If a coefficient ranges from L to U, then the multiplicative ratio is between \\(\\exp(L)\\) and \\(\\exp(U)\\); the % change is between \\(100 \\times (\\exp(L) - 1)\\) and \\(100 \\times (\\exp(U) - 1)\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_Model1.html",
    "href": "01_Model1.html",
    "title": "\n2  Fixed-effects location–scale model (model 1)\n",
    "section": "",
    "text": "2.1 Dataset overview\nModel 1 is a fixed-effects location-scale model, which allows us to model both the mean (location) and variance (scale) of a response variable simultaneously. This is particularly useful when we suspect that the variance of the response variable may differ across groups or treatments.\nThis dataset comes from a study by Cleasby et al. (2011), which investigated the effects of early-life food supplementation on adult morphology in a wild population of house sparrows (Passer domesticus). In particular, we focus on adult tarsus length as a measure of skeletal size. The dataset includes adult birds that either received supplemental food as chicks or not, and compares their tarsus length by treatment and sex.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fixed-effects location–scale model (model 1)</span>"
    ]
  },
  {
    "objectID": "01_Model1.html#dataset-overview",
    "href": "01_Model1.html#dataset-overview",
    "title": "\n2  Fixed-effects location–scale model (model 1)\n",
    "section": "",
    "text": "2.1.1 Questions\n\nDoes early-life food supplementation increase adult size? Specifically, does it increase the average tarsus length in adulthood?\nDoes early-life food supplementation lead to lower variation in adult tarsus length?\nAre there sex-specific effects of food supplementation? Do the effects on mean or variance differ between males and females?\n\nVariables included\nThe dataset includes adult birds that either received supplemental food as chicks or not, and compares their tarsus length by treatment and sex. We use the following variables:\n\n\n\n\n\n\n\n\nSex: Biological sex of the bird (“Male” or “Female”)\n\nAdTarsus: Adult tarsus length (mm)\n\nTreatment: Whether the bird received food supplementation as a chick (Fed) or not (Control)",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fixed-effects location–scale model (model 1)</span>"
    ]
  },
  {
    "objectID": "01_Model1.html#visualise-the-datasets",
    "href": "01_Model1.html#visualise-the-datasets",
    "title": "\n2  Fixed-effects location–scale model (model 1)\n",
    "section": "\n2.2 Visualise the datasets",
    "text": "2.2 Visualise the datasets\nThe plot shows how adult tarsus length varies by treatment (early-life food supplementation) and sex. Boxplots summarise the central tendency and spread, while the jitter points reveal the distribution of individual values. As shown in the plot, there is a clear tendency for reduced variability in the male treatment group (Fed), suggesting that early-life food supplementation may lead to more canalised development in males.\nIt should be noted that we applied a log-transformation to AdTarsus to reduce skewness and stabilise residual variance. Continuous variables are often log-transformed and mean-centred, but in location-scale models, we do not standardise the predictors, as doing so would be remove interpretable variation in the scale part of the model.\n\n# load the datasets　----\n\ndat_tarsus &lt;- read.csv(here(\"data\", \"SparrowTarsusData.csv\"), header = TRUE)\n\n#'*Added*\nggplot(dat_tarsus, aes(x = Treatment, y = log(AdTarsus), fill = Sex)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.5, position = position_dodge(width = 0.8)) +\n  geom_jitter(\n    aes(color = Sex),\n    size = 2, alpha = 0.7,\n    position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8)\n  ) +\n  scale_fill_manual(values = c(\"Male\" = \"#1f78b4\", \"Female\" = \"#e31a1c\")) + \n  scale_color_manual(values = c(\"Male\" = \"#1f78b4\", \"Female\" = \"#e31a1c\")) +\n  labs(title = \"Adult tarsus length by treatment and sex\",\n       x = \"Treatment\", y = \"Log-transformed tarsus length\") +\n  theme_classic() +\n  theme(legend.position = \"right\")",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fixed-effects location–scale model (model 1)</span>"
    ]
  },
  {
    "objectID": "01_Model1.html#run-models-and-interpret-results",
    "href": "01_Model1.html#run-models-and-interpret-results",
    "title": "\n2  Fixed-effects location–scale model (model 1)\n",
    "section": "\n2.3 Run models and interpret results",
    "text": "2.3 Run models and interpret results\nWe fit and compare two types of models to understand the structure of adult tarsus length:\n\nLocation-only model: Estimates the mean of adult tarsus length as a function of sex and early-life food supplementation (treatment).\nLocation-scale model: Estimates both the mean and the variability (residual dispersion) of adult tarsus length, allowing us to examine whether treatment and sex influence not only the average trait value but also its individual variation.\n\nThis approach enables us to detect subtle patterns, such as sex-specific canalisation, that may not be captured when modeling the mean alone.\n\n2.3.1 Model fitting\n\n\nLocation model\nResidual diagnostics\nLocation-scale model\nModel comparison\nSummary of model results\nbonus - brms\nResult figures\n\n\n\nFirst, we fit a location-only model as the baseline model.\n\n# location-only model ----\n\nmodel_0 &lt;- glmmTMB(\n    log(AdTarsus) ~ 1 + Sex + Treatment + Sex:Treatment,\n    data = dat_tarsus, \n    family = gaussian)\n\nsummary(model_0)\n\n Family: gaussian  ( identity )\nFormula:          log(AdTarsus) ~ 1 + Sex + Treatment + Sex:Treatment\nData: dat_tarsus\n\n     AIC      BIC   logLik deviance df.resid \n  -221.0   -210.2    115.5   -231.0       59 \n\n\nDispersion estimate for gaussian family (sigma^2): 0.00158 \n\nConditional model:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           2.88728    0.01064  271.42   &lt;2e-16 ***\nSexMale               0.01043    0.01373    0.76    0.447    \nTreatmentFed          0.01036    0.01436    0.72    0.471    \nSexMale:TreatmentFed  0.01353    0.02034    0.67    0.506    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## to quantify the uncertainty in parameter estimates, we computed 95% confidence intervals using the confint() function.\n## this function returns the lower and upper bounds for each fixed/random effect parameters. If a confidence interval does not include zero, it suggests that the corresponding predictor has a statistically significant effect (at approximately the 0.05 level).\n\nconfint(model_0) # check 95%CI\n\n                           2.5 %     97.5 %   Estimate\n(Intercept)           2.86643380 2.90813203 2.88728291\nSexMale              -0.01648352 0.03734866 0.01043257\nTreatmentFed         -0.01779914 0.03850937 0.01035511\nSexMale:TreatmentFed -0.02633701 0.05340220 0.01353260\n\n\n\n\nWe can check the residuals of the model to assess the model fit and assumptions. The Q-Q plot should show points falling along a straight line.\n\n# | label: model0_diagnostics - model1\n\n# plot a q-q plot of residuals to visually assess the normality assumption\n# the data points should fail approximately along the reference line\nres &lt;- residuals(model_0)\n\nqqnorm(res) # visual check for normality of residuals\nqqline(res) # reference line for normal distribution\n\n\n\n\n\n\n\nThe residuals mostly follow a straight line in the Q-Q plot, but there are some deviations, particularly at the lower and upper ends. This suggests that the model may not fully capture the distribution of the data, indicating some potential issues with normality or heteroscedasticity.\n\n\nThen, we fit a location-scale model.\n\n# location-scale model ---- \n\nmodel_1 &lt;- glmmTMB(\n    log(AdTarsus) ~ 1 + Sex + Treatment + Sex:Treatment, # location part\n    dispformula = ~ 1 + Sex + Treatment + Sex:Treatment, # scale part\n    data = dat_tarsus, \n    family = gaussian\n    )\nsummary(model_1)\n\n Family: gaussian  ( identity )\nFormula:          log(AdTarsus) ~ 1 + Sex + Treatment + Sex:Treatment\nDispersion:                     ~1 + Sex + Treatment + Sex:Treatment\nData: dat_tarsus\n\n     AIC      BIC   logLik deviance df.resid \n  -224.0   -206.7    120.0   -240.0       56 \n\n\nConditional model:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           2.88728    0.01006  286.87   &lt;2e-16 ***\nSexMale               0.01043    0.01343    0.78    0.437    \nTreatmentFed          0.01036    0.01566    0.66    0.508    \nSexMale:TreatmentFed  0.01353    0.01897    0.71    0.476    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -3.27919    0.18898 -17.352  &lt; 2e-16 ***\nSexMale               0.07846    0.24397   0.322  0.74775    \nTreatmentFed          0.27232    0.25520   1.067  0.28592    \nSexMale:TreatmentFed -0.95054    0.36139  -2.630  0.00853 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(model_1) \n\n                                2.5 %      97.5 %    Estimate\ncond.(Intercept)           2.86755650  2.90700935  2.88728293\ncond.SexMale              -0.01588533  0.03675044  0.01043255\ncond.TreatmentFed         -0.02033051  0.04104072  0.01035511\ncond.SexMale:TreatmentFed -0.02364145  0.05070669  0.01353262\ndisp.(Intercept)          -3.64959165 -2.90879561 -3.27919363\ndisp.SexMale              -0.39971908  0.55664476  0.07846284\ndisp.TreatmentFed         -0.22785555  0.77250192  0.27232319\ndisp.SexMale:TreatmentFed -1.65885084 -0.24223149 -0.95054117\n\n\n\n\nNow we can compare the two models to see if the location-scale model provides a better fit to the data than the location-only model. We can use the anova() function to compare the two models based on their AIC values. Alternatively, we can use the model.sel() function from the MuMIn package to compare AICc values, which is more appropriate for small sample sizes.\n\n# compare models ----\n## we can use the anova() function to compare the two models of AIC \nanova(model_0, model_1)\n\n\n  \n\n\n## model.sel() from the MuMIn package can be used to compare AICc values.\nmodel.sel(model_0, model_1)\n\n\n  \n\n\n\n\n\nThe results of the location-only model (Model 0) and the location-scale model (Model 1) are summarised below.\n\n\n\n\n\n\n\nLocation-only model\n\n\nTerm\nEstimate\nStd. Error\n95% CI (low)\n95% CI (high)\n\n\n\n\n(Intercept)\n2.887\n0.011\n2.866\n2.908\n\n\nSexMale\n0.010\n0.014\n−0.016\n0.037\n\n\nTreatmentFed\n0.010\n0.014\n−0.018\n0.039\n\n\nSexMale:TreatmentFed\n0.014\n0.020\n−0.026\n0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation-scale model (location part)\n\n\nTerm\nEstimate\nStd. Error\n95% CI (low)\n95% CI (high)\n\n\n\n\n(Intercept)\n2.887\n0.010\n2.868\n2.907\n\n\nSexMale\n0.010\n0.013\n−0.016\n0.037\n\n\nTreatmentFed\n0.010\n0.016\n−0.020\n0.041\n\n\nSexMale:TreatmentFed\n0.014\n0.019\n−0.024\n0.051\n\n\n\n\n\n\n\n\n\n\n\nLocation-scale model (dispersion part)\n\n\nTerm\nEstimate\nStd. Error\n95% CI (low)\n95% CI (high)\n\n\n\n\n(Intercept)\n−3.279\n0.189\n−3.650\n−2.909\n\n\nSexMale\n0.078\n0.244\n−0.400\n0.557\n\n\nTreatmentFed\n0.272\n0.255\n−0.228\n0.773\n\n\nSexMale:TreatmentFed\n−0.951\n0.361\n−1.659\n−0.242\n\n\n\n\n\n\n\n\nOf course, we can also fit the location–scale model using the brms! Here we show how to fit the same model as above using brms. The results should be similar to those obtained with glmmTMB.\n\n# specify the model using bf()\nformula1 &lt;- bf(\n  log(AdTarsus) ~  1 + Sex + Treatment + Sex:Treatment, \n  sigma = ~ 1 + Sex + Treatment + Sex:Treatment\n)\n\n# generate default priors based on the formula and data\ndefault_priors &lt;- default_prior(\n                        formula1,\n                        data = dat_tarsus,                             \n                        family = gaussian() # default link function for gaussian family                                 \n                          )\n\n# fit the model - you can change N of iter, warmup, thin, and also chains.\n# adapt_delta = 0.95 helps to reduce divergent transitions\nsystem.time(\n  brms_g1 &lt;- brm(formula1,\n                  data = dat_tarsus,           \n                  family = gaussian(),                   \n                  prior = default_priors,                \n                  iter = 2000,                          \n                  warmup = 1000, \n                  thin = 1,                              \n                  chains = 2,                            \n                  control = list(adapt_delta = 0.95) \n             )\n)\n\nsummary(brms_g1)\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: log(AdTarsus) ~ 1 + Sex + Treatment + Sex:Treatment \n         sigma ~ 1 + Sex + Treatment + Sex:Treatment\n   Data: dat_tarsus (Number of observations: 64) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                      2.89      0.01     2.86     2.91 1.00     1162\nsigma_Intercept               -3.19      0.21    -3.56    -2.72 1.00     1041\nSexMale                        0.01      0.01    -0.02     0.04 1.00      994\nTreatmentFed                   0.01      0.02    -0.03     0.05 1.00      795\nSexMale:TreatmentFed           0.01      0.02    -0.03     0.06 1.00      711\nsigma_SexMale                  0.04      0.27    -0.52     0.56 1.00     1025\nsigma_TreatmentFed             0.24      0.28    -0.30     0.77 1.00      917\nsigma_SexMale:TreatmentFed    -0.87      0.41    -1.66    -0.01 1.00      890\n                           Tail_ESS\nIntercept                      1130\nsigma_Intercept                1202\nSexMale                        1040\nTreatmentFed                    756\nSexMale:TreatmentFed            865\nsigma_SexMale                  1079\nsigma_TreatmentFed             1082\nsigma_SexMale:TreatmentFed     1050\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFirst, you need to check the effective sample size (**_ESS) and Rhat values. ESS should be greater than 400 (Vehtari et al. 2021), and \\(\\hat{R}\\) should be close to 1.0 (ideally &lt; 1.01). If these conditions are not met, you may need to increase the number of iterations or adjust the model specification.\nThen, we can check the output. It is divided into two parts: Location (mean) part (how the average changes) and Scale (dispersion) part (how the variability changes). In Gaussian data, the scale part is \\(\\sigma\\)\nHere is the explanation of the output table:\n\nEstimate: posterior mean\nEst.Err: standard error of posterior mean\nl-95% CI and u-95% CI: Lower and upper bounds of the 95% credible interval (range where the true value lies with 95% probability, given the model and data)\nRhat: Convergence diagnostic. Should be close to 1.00. If &gt;1.01, convergence may be poor.\nBulk_ESS and Tail_ESS: Effective sample sizes for bulk and tail distributions. Should be &gt;400 for reliable estimates (larger is better).\n\nWe evaluated model fit using posterior predictive checks implemented in the brms functionpp_check(). This procedure generates replicated datasets from the posterior distribution of the fitted model and compares them with the observed data. If the model provides an adequate description of the data, the replicated distributions should resemble the observed distribution in both central tendency and variability.\n\npp_check(brms_g1) # posterior predictive check\n\n\n\n\n\n\n\nPosterior predictive checks indicated that the Gaussian location–scale model adequately reproduced the observed distribution of log(AdTarsus). In the plots, the dark line shows the observed data, while the lighter lines represent simulated datasets drawn from the posterior predictive distribution. Close overlap between observed and replicated distributions indicates that the model reproduces the data well. Deviations suggest areas where the model fails to capture features of the data (e.g. heavy tails, skewness, or multimodality).\n\nNow, you can find the results from glmmTMB and brms are very close to each other, but there are some differences in the estimates and standard errors. This came from the different estimation methods used by the two packages. glmmTMB uses maximum likelihood estimation, while brms uses Bayesian estimation with Markov Chain Monte Carlo (MCMC) sampling.\n\n\nThe figure visualises results from the glmmTMB and brms models shown on the link scale.\n\n# TODO: change colours and maybe set figure size\n\n# glmmTMB ----\n# Extract estimated marginal means for location part\n# returns predicted log(AdTarsus) by Sex*Treatment\nemm_mu_link &lt;- emmeans(model_1, ~ Sex * Treatment,\n                       component = \"cond\", type = \"link\")\ndf_mu_link &lt;- as.data.frame(emm_mu_link) %&gt;%\n  rename(\n    mean_log = emmean, # estimated log(mean AdTarsus)\n    lwr = lower.CL, # lower 95% CI (log scale)\n    upr = upper.CL # upper 95% CI (log scale)\n  )\n\n# Extract estimated marginal means for scale part \nemm_sigma_link &lt;- emmeans(model_1, ~ Sex * Treatment,\n                          component = \"disp\", type = \"link\")\ndf_sigma_link &lt;- as.data.frame(emm_sigma_link) %&gt;%\n  rename(\n    log_sigma = emmean, # estimated log(sigma)\n    lwr = lower.CL, # lower 95% CI (log scale)\n    upr = upper.CL # upper 95% CI (log scale)\n  )\n\npos &lt;- position_dodge(width = 0.4)\n\n# Plot predicted log(AdTarsus) ± 95% CI for Sex*Treatment\np_mean_link &lt;- ggplot(df_mu_link, aes(x = Sex, y = mean_log, color = Treatment)) +\n  geom_point(position = pos, size = 3) +\n  geom_errorbar(aes(ymin = lwr, ymax = upr), position = pos, width = 0.15) +\n  labs(\n    title = \"Location\",\n    x = \"Sex\",\n    y = \" log(AdTarsus)\"\n  ) +\n  scale_y_continuous(limits = c(2.8, 3.0), breaks = seq(2.8, 3.0, by = 0.1) ) + \n  theme_classic()\n\n# Plot predicted log(sigma) ± 95% CI for Sex*Treatment\np_sigma_link &lt;- ggplot(df_sigma_link, aes(x = Sex, y = log_sigma, color = Treatment)) +\n  geom_point(position = pos, size = 3) +\n  geom_errorbar(aes(ymin = lwr, ymax = upr), position = pos, width = 0.15) +\n  labs(\n    title = \"Scale\",\n    x = \"Sex\",\n    y = \"log(sigma)\"\n  ) +\n  scale_y_continuous(limits = c(-4.5, -2.5), breaks = seq(-4.5, -2.5, by = 0.5) ) + \n  theme_classic()\n\n# scale_data &lt;- log(sqrt(residuals(model_1)^2)) # this is the residual  - we can add the figure, but it does not come from the model\n     \np_glmmTMB_m1 &lt;- p_mean_link + p_sigma_link\n\n# brms ----\nd &lt;- dat_tarsus %&gt;%\n  distinct(Sex, Treatment) %&gt;%\n  arrange(Sex, Treatment)\n\ndraws_link &lt;- tidybayes::linpred_draws(\n  brms_g1, newdata = d, re_formula = NA, dpar = TRUE, transform = FALSE\n) %&gt;%\n  transmute(\n    Sex, Treatment, .draw,\n    mu = .linpred,  # Location on link scale\n    log_sigma = sigma  # Scale on link scale (already log(sigma))\n  ) %&gt;%\n  tidyr::pivot_longer(mu:log_sigma, names_to = \"param\", values_to = \"value\")\n\n# Location ----\np_loc &lt;- draws_link %&gt;%\n  filter(param == \"mu\") %&gt;%\n  ggplot(aes(x = Sex, y = value, fill = Treatment)) +\n  geom_violin(width = 0.85, trim = FALSE, alpha = 0.6,\n              color = NA, position = position_dodge(0.6)) +\n  stat_pointinterval(aes(color = Treatment),\n                                position = position_dodge(width = 0.6),\n                                .width = 0.95, size = 0.8) +\n  scale_y_continuous(limits = c(2.8, 3.0), breaks = seq(2.8,3.0, by = 0.1) ) + \n  labs(title = \"Location\", x = \"Sex\", y = \"log(AdTarsus)\") +\n  theme_classic() +\n  theme(legend.position = \"right\")\n\n# Scale ----\np_scl &lt;- draws_link %&gt;%\n  filter(param == \"log_sigma\") %&gt;%\n  ggplot(aes(x = Sex, y = value, fill = Treatment)) +\n  geom_violin(width = 0.85, trim = FALSE, alpha = 0.6,\n              color = NA, position = position_dodge(0.6)) +\n  tidybayes::stat_pointinterval(aes(color = Treatment),\n                                position = position_dodge(width = 0.6),\n                                .width = 0.95, size = 0.8) +\n  labs(title = \"Scale\", x = \"Sex\", y = \"log(sigma)\") +\n  theme_classic() +\n  theme(legend.position = \"right\")\n\np_brms_m1 &lt;- p_loc + p_scl\n# combine ----\n(p_glmmTMB_m1 / p_brms_m1) +\n  plot_annotation(title = \"top: glmmTMB / bottom: brms\")\n\n\n\n\n\n\n\n\n\nLocation (mean) part: The glmmTMB panel (top) shows estimated marginal means (EMMs) with 95% confidence intervals, represented as points with error bars, obtained via emmeans. The brms panel (bottom) displays posterior samples drawn with tidybayes::linpred_draws() as violin plots, overlaid with the posterior median and 95% credible intervals. Thus, the top panel presents point estimates with symmetric CIs based on normal approximation, while the bottom panel visualises the full posterior distribution, including its shape (skewness and tail thickness). Both use type = \"link\", so the vertical axis is aligned on the log(AdTarsus) link scale.\n\nScale (variance) part: The plots from glmmTMB model display Estimated marginal means of log(sigma) with 95% confidence intervals. the plots from brms show posterior distributions, medians, and 95% credible intervals for log(sigma).\n\nInterpretation of uncertainty intervals: A confidence interval (CI) has a frequentist interpretation: if the data were replicated infinitely, 95% of such intervals would contain the true value. A credible interval (CrI) has a Bayesian interpretation: given the data and the prior, there is a 95% probability that the true value lies within the interval.\n\nIn principle, raw data could also be plotted for both the location and scale parts (e.g., observed values or residual variability), but these would not represent model-derived estimates. Therefore, only group-level estimates and their uncertainty are shown here.\n\n\n\n\n2.3.2 Comparison of location-only model and location-scale model\nThere was no significant difference in the fit of the two models. location-scale model (model 1) had a lower AICc (-221.0) than location-only model (model 0: -220.0), with an AIC weight of 0.662 vs. 0.338 (see Model comparison tab).　\n\n\n\n\n\n\nNote on AIC vs AICc:\n\n\n\nWhile both AIC (Akaike Information Criterion) and AICc (AIC with correction) assess model fit by balancing goodness of fit and model complexity, AICc includes an additional correction for small sample sizes. When the sample size is limited relative to the number of estimated parameters, AICc is generally preferred because it reduces the risk of overfitting. As a result, AIC and AICc values may differ slightly, and model rankings based on them may also vary.\n\n\n\n2.3.3 Interpretation of location-scale model :\n\n2.3.3.1 How to back-transflrm the log-link scale to natural scale?\nThe models were fitted with different link functions for the location and scale submodels. Estimates were therefore back-transformed before biological interpretation. For the location part, the link was identity, so predicted values of log(AdTarsus) were exponentiated to return to millimetres of adult tarsus length. For the scale part, the link was logarithmic, so estimates of log(sigma) were exponentiated to obtain the residual standard deviation (sigma) on the original scale. To facilitate interpretation, group differences in means are expressed as percentage changes relative to the control females, and group differences in residual variation are expressed as proportional changes in σ (i.e. percentage reduction or increase compared with the reference group).\nFor example…\n\nLocation part\n\n\nfemales with control: exp(2.89) = 17.99\nmales with control: exp(2.89 + 0.01) = 18.17\nif we want to know the percentage difference between females vs males with control… exp(0.01) - 1 = +1.0%\nfemales with fed: exp(2.89 + 0.01) = 18.17\nmales with fed: exp(2.89 + 0.01 + 0.01 + 0.01) = 18.54\n\n\nScale part\n\n\nmales with control: log(sigma) = exp(-3.19 + 0.04) = 0.043\n\n\n2.3.3.2 Biological meanings\nLocation (mean) part:\n\nAverage adult tarsus length was around 18 mm in the control females (back-transformed: SexFemale; \\(\\beta_{[\\text{intercept}]}^{(l)}\\) = -3.28). Neither food supplementation nor sex produced more than ~2-4% differences in mean length: males were estimated to be −1.6% to +3.7% longer than females, supplemented birds were −2.0% to +4.2% longer than controls, and supplemented males were −2.3% to +5.2% longer than control males. These small and uncertain differences indicate that food supplementation did not meaningfully alter average adult tarsus length.\n\nScale (dispersion) part:  - There was a significant negative interaction between sex and treatment (SexMale:TreatmentFed; \\(\\beta_{[\\text{interaction}]}^{(s)} = -0.95\\), 95% CI = -1.66, -0.24 in glmmTMB). Back-transformation indicated that the residual standard deviation in supplemented males was 61.3% lower than that in the baseline group (non-supplemented females; 95% CI: −81.0% to −21.3%) and 58.2% lower than in non-supplemented males. Thus, early-life food supplementation substantially reduced variation in adult tarsus length among males, resulting in more uniform growth. Neither sex nor treatment alone had a significant effect on variance; the reduction was specific to supplemented males.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fixed-effects location–scale model (model 1)</span>"
    ]
  },
  {
    "objectID": "01_Model1.html#conclusion",
    "href": "01_Model1.html#conclusion",
    "title": "\n2  Fixed-effects location–scale model (model 1)\n",
    "section": "\n2.4 Conclusion",
    "text": "2.4 Conclusion\nQ1. Does early-life food supplementation increase adult size? Specifically, does it increase the average tarsus length in adulthood?\nAnswer: No clear evidence. In both the location-only and location-scale models, the effect of feeding (treatment) on the mean adult tarsus length was small and not statistically significant. This suggests that food supplementation did not lead to a measurable increase in average tarsus length.\nQ2. Does early-life food supplementation lead to lower variation in adult tarsus length??\nAnswer: Partially yes - especially in males. The location-scale model revealed a significant reduction in variance in the male treatment group (Fed) compared to the male control group. This was supported by a significant negative interaction between sex and treatment in the dispersion model. In contrast, females showed no significant difference in variance between treatment groups. This indicates that early-life food supplementation reduced size variation only in males, not across all individuals.\nQ3. Are there sex-specific effects of food supplementation? Do the effects on mean or variance differ between males and females?\nAnswer: Yes. The male treatment group (Fed) showed significantly reduced variance in adult tarsus length compared to the control group (Control), while females did not show a significant difference in variance between treatment groups. There were no significant differences in mean tarsus length between sexes or treatments. This pattern suggests that early-life food supplementation may canalise trait development in males, leading to more uniform adult morphology under favourable nutritional conditions.\n\nAlthough the model comparison did not show a strong difference in overall fit between the location-only and location-scale models (\\(\\Delta AICc = 1.3\\)), the location-scale model revealed an important and previously overlooked pattern:\nEarly-life food supplementation significantly reduced trait variance in males, but not in females.\nThis result would have been missed in a traditional location-only analysis that focuses solely on mean differences. By modeling both the mean and the dispersion, we were able to detect a sex-specific canalisation effect, highlighting the value of using location-scale models when investigating trait variability and developmental plasticity.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fixed-effects location–scale model (model 1)</span>"
    ]
  },
  {
    "objectID": "02_Model2.html",
    "href": "02_Model2.html",
    "title": "\n3  Adding random effects in the location part only (model 2)\n",
    "section": "",
    "text": "3.1 Dataset overview\nModel 2 is a location-scale model with random effects in the location part only. This allows us to account for individual-level variation in the mean response while still modeling the dispersion separately.\nThis dataset comes from Drummond et al (2025), which examined the benefits of brood reduction in the blue-footed booby (Sula nebouxii).Focusing on two-chick broods, the research explored whether the death of one chick (brood reduction) primarily benefits the surviving sibling (via increased resource acquisition) or the parents (by lessening parental investment). In this species, older chicks establish a strong dominance hierarchy over their younger siblings. Under stressful environmental conditions, this often results in the death of the second-hatched chick, with parental intervention being extremely rare.\nFor this tutorial, we will specifically examine data from two-chick broods where both chicks survived to fledgling. Our analysis will focus on how hatching order influences the body condition of chicks at fledgling.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Adding random effects in the location part only (model 2)</span>"
    ]
  },
  {
    "objectID": "02_Model2.html#dataset-overview",
    "href": "02_Model2.html#dataset-overview",
    "title": "\n3  Adding random effects in the location part only (model 2)\n",
    "section": "",
    "text": "3.1.1 Questions\n\nDo first hatched chicks have a higher body mass than second-hatched chicks?\nDoes the variability in body mass differ between first and second-hatched chicks?\n\n3.1.2 Variables included\n\n\n\n\n\n\n\n\nSMI: Scaled Mass Index, a measure that accounts for body mass relative to size\n\nRANK: The order in which chicks hatched (1 for first-hatched, 2 for second-hatched)\n\nNEST_ID: Identifier for the nest where the chicks were raised\n\nYEAR: The year in which the chicks were born",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Adding random effects in the location part only (model 2)</span>"
    ]
  },
  {
    "objectID": "02_Model2.html#visualise-the-dataset",
    "href": "02_Model2.html#visualise-the-dataset",
    "title": "\n3  Adding random effects in the location part only (model 2)\n",
    "section": "\n3.2 Visualise the dataset",
    "text": "3.2 Visualise the dataset\nThe plot displays the distribution of body condition, measured as Scaled Mass Index (SMI), for blue-footed booby chicks based on their hatching order. Violin plots illustrate the overall density distribution of SMI for both “First-Hatched” and “Second-Hatched” chicks. Each individual empty circle represents the SMI of a single chick. The plot visually highlights individual variability and allows for the comparison of both central tendency (black solid line) and spread of body condition between the two hatching orders.\n\ndat &lt;- read_csv(here(\"data\",\"SMI_Drummond_et_al_2025.csv\")) %&gt;%subset(REDUCTION == 0)\n\ndat&lt;- dat%&gt;%\n  dplyr::select(-TIME, -HATCHING_DATE,-REDUCTION,-RING) %&gt;%\n  mutate(SMI = as.numeric(SMI),# Scaled mass index\n         lnSMI=log(SMI),\n         NEST = as.factor(NEST), \n         WORKYEAR = as.factor(WORKYEAR),\n         RANK = factor(RANK, levels = c(\"1\", \"2\")))\n\n\nggplot(dat, aes(x = RANK, y = SMI, fill = RANK, color = RANK)) +\n geom_violin(aes(fill = RANK),\n              color = \"#8B8B83\",\n              width = 0.8, \n              alpha = 0.3,\n              position = position_dodge(width = 0.7)) +\n  geom_jitter(aes(color = RANK),\n              size = 3,\n              alpha = 0.4,\n              shape = 1,\n              position = position_jitterdodge(dodge.width = 0.5, jitter.width = 0.15)) + \n  stat_summary(fun = mean,              \n               geom = \"crossbar\",       \n               width = 0.1,             \n               color = \"black\",         \n               linewidth = 0.5,         \n               position = position_dodge(width = 0.7))+\n  labs(\n    title = \"Body Condition (SMI) by Hatching Order\",\n    x = \"Hatching Order\",\n    y = \"Scaled Mass Index (g)\"\n  ) +\n  scale_fill_manual(\n    values = c(\"1\" = \"#1F78B4\", \"2\" = \"#E31A1C\") # \n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#1F78B4\", \"2\" = \"#E31A1C\") # \n  ) +\n  scale_x_discrete(\n    labels = c(\"1\" = \"First-Hatched\", \"2\" = \"Second-Hatched\"),\n    expand = expansion(add = 0.5)\n  ) +\n  theme_classic(base_size = 16) +\n  theme(\n    axis.text = element_text(color = \"#6E7B8B\", size = 14),\n    axis.title = element_text(color = \"#6E7B8B\", size = 14),\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 0, hjust = 0.5)\n  )",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Adding random effects in the location part only (model 2)</span>"
    ]
  },
  {
    "objectID": "02_Model2.html#run-models-and-interpret-results",
    "href": "02_Model2.html#run-models-and-interpret-results",
    "title": "\n3  Adding random effects in the location part only (model 2)\n",
    "section": "\n3.3 Run models and interpret results",
    "text": "3.3 Run models and interpret results\nAll models incorporate NEST_ID and YEAR random effects to account for non-independence of chicks within the same nest and observations within the same year.\nThe models are as follows:\n\nLocation-only model: Estimates the average difference in Scaled Mass Index (SMI) between the two hatching orders.\nLocation-scale model: Estimates both the average difference (location) and differences in the variability (scale) of SMI between hatching orders.\n\n\n\nLocation model\nModel diagnostics\nLocation-scale model\nModel comparison\nSummary of model results\nFigures\n\n\n\nFollowing the previous section, we first fit a location-only model as our baseline and applied a log-transformation to SMI.\n\nmodel2_1&lt;-glmmTMB(lnSMI ~ 1 + RANK + (1|NEST)+(1|WORKYEAR),\n              family = gaussian(), data=dat)\n\n\n\n\n\n\n\nRationale for Log-Transforming SMI Transforming the dependent variable to \\(log(SMI)\\) is a standard approach used here for three primary reasons:\nTo Normalize Residuals: Mass data are typically right-skewed. The log transformation helps symmetrize the data’s distribution, making the model’s residuals better approximate the Gaussian distribution required by the family.\nTo Linearize Relationships: The log transform converts potentially non-linear, multiplicative relationships (common in biology) into the linear, additive relationships that are the foundation of this modeling framework.\nTo Address Heteroscedasticity: This transformation can help reduce heteroscedasticity, where variance increases with the mean. It’s a common first step to stabilize variance, though we will explicitly model the dispersion in subsequent models rather than relying solely on this transformation.\n\nCodeggplot(dat, aes(x = RANK, y = lnSMI, fill = RANK, color = RANK)) +\n geom_violin(aes(fill = RANK),\n              color = \"#8B8B83\",\n              width = 0.8, \n              alpha = 0.3,\n              position = position_dodge(width = 0.7)) +\n  geom_jitter(aes(color = RANK),\n              size = 3,\n              alpha = 0.4,\n              shape = 1,\n              position = position_jitterdodge(dodge.width = 0.5, jitter.width = 0.15)) + \n  stat_summary(fun = mean,              \n               geom = \"crossbar\",       \n               width = 0.1,             \n               color = \"black\",         \n               linewidth = 0.5,         \n               position = position_dodge(width = 0.7))+\n  labs(\n    title = \"ln(SMI) by Hatching Order\",\n    x = \"Hatching Order\",\n    y = \"Scaled Mass Index (g)\"\n  ) +\n  scale_fill_manual(\n    values = c(\"1\" = \"#1F78B4\", \"2\" = \"#E31A1C\") # \n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#1F78B4\", \"2\" = \"#E31A1C\") # \n  ) +\n  scale_x_discrete(\n    labels = c(\"1\" = \"First-Hatched\", \"2\" = \"Second-Hatched\"),\n    expand = expansion(add = 0.5)\n  ) +\n  theme_classic(base_size = 16) +\n  theme(\n    axis.text = element_text(color = \"#6E7B8B\", size = 14),\n    axis.title = element_text(color = \"#6E7B8B\", size = 14),\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 0, hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nsummary(model2_1)\n\n Family: gaussian  ( identity )\nFormula:          lnSMI ~ 1 + RANK + (1 | NEST) + (1 | WORKYEAR)\nData: dat\n\n     AIC      BIC   logLik deviance df.resid \n -8048.8  -8016.5   4029.4  -8058.8     4732 \n\nRandom effects:\n\nConditional model:\n Groups   Name        Variance Std.Dev.\n NEST     (Intercept) 0.002434 0.04934 \n WORKYEAR (Intercept) 0.008858 0.09412 \n Residual             0.008245 0.09080 \nNumber of obs: 4737, groups:  NEST, 2873; WORKYEAR, 24\n\nDispersion estimate for gaussian family (sigma^2): 0.00824 \n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  7.35953    0.01973   372.9  &lt; 2e-16 ***\nRANK2       -0.01872    0.00272    -6.9 5.91e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(model2_1)\n\n                                   2.5 %      97.5 %    Estimate\n(Intercept)                   7.32085592  7.39820911  7.35953252\nRANK2                        -0.02405065 -0.01338785 -0.01871925\nStd.Dev.(Intercept)|NEST      0.04463325  0.05454148  0.04933927\nStd.Dev.(Intercept)|WORKYEAR  0.07008890  0.12638674  0.09411859\n\n\n\n\n\n\n\n\nInterpreting Log-Level Model Coefficients\n\n\n\nSince the outcome is logged, this is a log-level model. A coefficient for a categorical predictor like RANK represents a multiplicative change on the original SMI scale.\nTo interpret a coefficient (\\(\\beta\\)) for a given level of RANK relative to the reference level, you must exponentiate it.\n\n\nMultiplicative Factor on SMI: \\[\n  \\text{Factor} = e^{\\beta}\n  \\]\n\n\nPercentage Change in SMI: \\[\n  \\text{Change} = (e^{\\beta} - 1) \\times 100\\%\n  \\]\n\n\n\n3.3.0.1 Example:\nOur model estimates the coefficient for RANK = ‘junior’ to be -0.018 (with ‘senior’ as the reference level):\n\n\nCalculate the multiplicative factor: \\[\ne^{-0.018} \\approx 0.982\n\\]\n\n\nConvert to a percentage change: \\[\n(0.982 - 1) \\times 100\\% = -1.7\\%\n\\]\n\n\nThe concise interpretation is:\n“The scaled mass index for second-hatched individuals is, on average, 1.7% lower than that of the first-hatched reference group.”\n\n\n\n\n\nThen we use the ‘DHARMa’ package to simulate residuals and plot them automatically. This step is crucial for checking the model’s assumptions, such as normality and homoscedasticity of residuals.\n\n# | label: model_diagnostics - model0\nsimulationOutput &lt;- simulateResiduals(fittedModel = model2_1, plot = F)\nplot(simulationOutput)\n\n\n\n\n\n\n\nThe QQ plot and Kolmogorov-Smirnov (KS) test reveal that the model’s residuals aren’t uniformly distributed, hinting at potential issues with the chosen distribution or the model’s underlying structure. While tests for overdispersion and outliers showed no significant concerns, the boxplots and Levene test highlight a lack of uniformity within residual groups and non-homogeneous variances. This suggests the model doesn’t fully capture the data’s complexity and might benefit from adjustments, perhaps by incorporating a location-scale model to better handle varying dispersion.\n\n\n\n# location-scale model ---- \n\nmodel2_2 &lt;- glmmTMB(\n    lnSMI ~ 1 + RANK + (1|NEST)+(1|WORKYEAR),\n    dispformula = ~ 1 + RANK,     \n    data = dat, \n    family = gaussian\n    )\nsummary(model2_2)\n\n Family: gaussian  ( identity )\nFormula:          lnSMI ~ 1 + RANK + (1 | NEST) + (1 | WORKYEAR)\nDispersion:             ~1 + RANK\nData: dat\n\n     AIC      BIC   logLik deviance df.resid \n -8070.8  -8032.1   4041.4  -8082.8     4731 \n\nRandom effects:\n\nConditional model:\n Groups   Name        Variance Std.Dev.\n NEST     (Intercept) 0.002430 0.04929 \n WORKYEAR (Intercept) 0.008595 0.09271 \n Residual                   NA      NA \nNumber of obs: 4737, groups:  NEST, 2873; WORKYEAR, 24\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  7.36075    0.01944   378.7  &lt; 2e-16 ***\nRANK2       -0.01880    0.00271    -6.9 3.93e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.47026    0.02242 -110.20  &lt; 2e-16 ***\nRANK2        0.13035    0.02687    4.85 1.23e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(model2_2)\n\n                                        2.5 %      97.5 %    Estimate\ncond.(Intercept)                   7.32265786  7.39885037  7.36075412\ncond.RANK2                        -0.02411510 -0.01349325 -0.01880417\ndisp.(Intercept)                  -2.51419214 -2.42631943 -2.47025578\ndisp.RANK2                         0.07768165  0.18301526  0.13034846\ncond.Std.Dev.(Intercept)|NEST      0.04462552  0.05445269  0.04929482\ncond.Std.Dev.(Intercept)|WORKYEAR  0.06903183  0.12450065  0.09270657\n\n\n\n\n\n\n\n\nInterpreting Location-Scale Model Coefficients\n\n\nLocation Model: Mean (Conditional model)\nScale Model: Variability (Dispersion model)\n\n\n\nThis component predicts the mean of \\(log(SMI)\\). Because it’s on the log scale, coefficients translate into percentage changes in SMI.\nRANK2 Coefficient = -0.01880\nMultiplicative factor: \\(e^{-0.01880} \\approx 0.981\\)\nPercentage change: \\(-1.86%\\)\nInterpretation: Individuals in RANK2 have, on average, a 1.9% lower SMI than those in RANK1.\n\n\nThis component models the residual standard deviation (\\(σ\\)) of log(SMI). Since \\(σ\\) is constrained to be positive, the model uses a log link, so coefficients are exponentiated and interpreted as percentage changes in variability.\nRANK2 Coefficient = 0.13035\nMultiplicative factor: \\(e^{0.13035} \\approx 1.14\\)\nPercentage change: \\(+14.0%\\)\nInterpretation: The residual variability in log(SMI) for RANK2 individuals is 14% higher than for RANK1, meaning their SMI values are more variable.\n\n\n\n\n\n\n\n\nFinally, we compare the location-scale model with the location-only model based on AICc values (model.sel() from the MuMIn package).\n\nmodel.sel(model2_1, model2_2)\n\n\n  \n\n\n\n\n\nThe results of the location-only model (Model 0) and the location-scale model (Model 1) are summarized below.\n\n\n\n\n\n\n\nLocation-only model\n\n\nTerm\nEstimate\nStd. Error\n95% CI (low)\n95% CI (high)\n\n\n\n\n(Intercept)\n7.360\n0.020\n7.321\n7.398\n\n\nRANK2\n−0.019\n0.003\n−0.024\n−0.013\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation-scale model (location part)\n\n\nTerm\nEstimate\nStd. Error\n95% CI (low)\n95% CI (high)\n\n\n\n\n(Intercept)\n7.361\n0.019\n7.323\n7.399\n\n\nRANK2\n−0.019\n0.003\n−0.024\n−0.013\n\n\n\n\n\n\n\n\n\n\n\nLocation-scale model (dispersion part)\n\n\nTerm\nEstimate\nStd. Error\n95% CI (low)\n95% CI (high)\n\n\n\n\n(Intercept)\n−2.470\n0.022\n−2.514\n−2.426\n\n\nRANK2\n0.130\n0.027\n0.078\n0.183\n\n\n\n\n\n\n\n\n\nemm_location &lt;- emmeans(model2_2, ~ RANK,\n                        component = \"cond\", type = \"link\")\ndf_location &lt;- as.data.frame(emm_location) %&gt;%\n  rename(\n    mean_log = emmean, \n    lwr = lower.CL,    \n    upr = upper.CL     \n  )\nemm_dispersion &lt;- emmeans(model2_2, ~ RANK,\n                          component = \"disp\", type = \"response\")\ndf_dispersion &lt;- as.data.frame(emm_dispersion) %&gt;%\n  rename(\n    log_sigma = response, \n    lwr = lower.CL,     \n    upr = upper.CL     \n  )\npos &lt;- position_dodge(width = 0.4)\n\n\nplot_location_emmeans &lt;- ggplot(df_location, aes(x = RANK, y = mean_log)) +\n  geom_point(position = pos, size = 3) +\n  geom_errorbar(aes(ymin = lwr, ymax = upr), position = pos, width = 0.15) +\n  labs(\n    title = \"Location\",\n    x = \"Hatching order\",\n    y = \"log(SMI)\"\n  ) +\n  theme_classic()+ scale_y_continuous(breaks = seq(7.2,7.5,0.05),limits = c(7.2, 7.5))\n\nplot_dispersion_emmeans &lt;- ggplot(df_dispersion, aes(x = RANK, y = log_sigma)) +\n  geom_point(position = pos, size = 3) +\n  geom_errorbar(aes(ymin = lwr, ymax = upr), position = pos, width = 0.15) +\n  labs(\n    title = \"Scale\",\n    x = \"Hatching order\",\n    y = \"Standard Deviation\"\n  ) +\n  theme_classic() + scale_y_continuous(breaks = seq(0.07,0.12,0.01),limits = c(0.07, 0.12))\n\nplot_location_emmeans + plot_dispersion_emmeans",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Adding random effects in the location part only (model 2)</span>"
    ]
  },
  {
    "objectID": "02_Model2.html#bonus",
    "href": "02_Model2.html#bonus",
    "title": "\n3  Adding random effects in the location part only (model 2)\n",
    "section": "\n3.4 Bonus",
    "text": "3.4 Bonus\nHere is how to fit the same location-scale models using the brms package for comparison.\n\n\nLocation-only model\nLocation-scale model\nModel comparison\nSummary of brms model results\nComparison of location-only model and location-scale model\nFigures brms models\n\n\n\n\n\nm1 &lt;- bf(log(SMI) ~ 1 + RANK + (1|NEST) + (1|WORKYEAR))\nprior1&lt;-default_prior(m1, data = dat, family = gaussian())\n\nfit1 &lt;- brm(\n  m1,\n  prior = prior1,\n  data = dat,\n  family = gaussian(),\n  iter = 6000,     \n  warmup = 1000,   \n  chains = 4,  cores=4,\n  backend = \"cmdstanr\",\n  control = list(\n    adapt_delta = 0.99,  # Keep high if you have divergent transitions\n    max_treedepth = 15   # Keep high if hitting max_treedepth warnings\n    ),\n  seed = 123,      \n  refresh = 500    # Less frequent progress updates (reduces overhead)\n)\n\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: log(SMI) ~ (1 | NEST) + (1 | WORKYEAR) + RANK \n   Data: dat (Number of observations: 4737) \n  Draws: 4 chains, each with iter = 6000; warmup = 1000; thin = 1;\n         total post-warmup draws = 20000\n\nMultilevel Hyperparameters:\n~NEST (Number of levels: 2873) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.05      0.00     0.04     0.05 1.00     4028     7765\n\n~WORKYEAR (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.10      0.02     0.08     0.14 1.00     5145     9062\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     7.36      0.02     7.32     7.40 1.00     2947     5200\nRANK2        -0.02      0.00    -0.02    -0.01 1.00    25883    12974\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.09      0.00     0.09     0.09 1.00     5732    10919\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nm2 &lt;- bf(log(SMI) ~ 1 + RANK + (1|NEST) + (1|WORKYEAR),\n         sigma~ 1 + RANK)\nprior2 &lt;- default_prior(m2,data = dat,family = gaussian())\n\nfit2 &lt;- brm(\n m2,\n prior= prior2,\n data = dat,\n family = gaussian(),\n iter = 6000,     \n warmup = 1000,   \n chains = 4,  cores=4,\n backend = \"cmdstanr\",\n control = list(\n   adapt_delta = 0.99,  # Keep high if you have divergent transitions\n   max_treedepth = 15   # Keep high if hitting max_treedepth warnings\n ),\n seed = 123,      \n refresh = 500    # Less frequent progress updates (reduces overhead)\n)\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: log(SMI) ~ 1 + RANK + (1 | NEST) + (1 | WORKYEAR) \n         sigma ~ 1 + RANK\n   Data: dat (Number of observations: 4737) \n  Draws: 4 chains, each with iter = 6000; warmup = 1000; thin = 1;\n         total post-warmup draws = 20000\n\nMultilevel Hyperparameters:\n~NEST (Number of levels: 2873) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.05      0.00     0.04     0.05 1.00     3871     8155\n\n~WORKYEAR (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.10      0.02     0.07     0.14 1.00     3937     7503\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           7.36      0.02     7.32     7.40 1.00     1826     4007\nsigma_Intercept    -2.47      0.02    -2.51    -2.43 1.00     5963    11191\nRANK2              -0.02      0.00    -0.02    -0.01 1.00    31665    14172\nsigma_RANK2         0.13      0.03     0.08     0.18 1.00    16024    15761\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\nf1loo &lt;- loo::loo(fit1)\nf2loo &lt;- loo::loo(fit2)\n\n#Model comparison \nfc &lt;- loo::loo_compare(f1loo, f2loo)\nfc\n\n#     elpd_diff se_diff\n# fit2   0.0       0.0  \n# fit1 -10.3       7.6  \n\nLOO (Leave-One-Out) model comparison helps us estimate which Bayesian model will best predict new data. The elpd_diff (Estimated Log Predictive Density difference) tells us the difference in predictive accuracy, with a more positive number indicating better performance. The se_diff (Standard Error of the Difference) provides the uncertainty around this difference. In our results, fit2 (the location-scale model) has an elpd_diff of 0.0, meaning it’s the reference or best-performing model. fit1 (the location-only model) has an elpd_diff of -10.3 with an se_diff of 7.6, indicating that fit2 is estimated to be substantially better at predicting new data than fit1, although there is some uncertainty in that difference. As a rule of thumb, if the absolute value of the elpd_diff is more than twice the se_diff, we can consider the difference as meaningful (\\(|elpd\\_diff| &gt; 2 * se\\_diff\\)).\n\n\n\n\n\n\n\n\n\n\nModel 1: Posterior Summary\n\n\nTerm\nEstimate\nStd.error\n95% CI (low)\n95% CI (high)\n\n\n\n\nLocation Model\n\n\nIntercept\n7.359\n0.022\n7.316\n7.403\n\n\nSecond hatched chick\n−0.019\n0.003\n−0.024\n−0.013\n\n\nRandom Effects\n\n\nNest ID\n0.049\n0.003\n0.044\n0.054\n\n\nYear\n0.102\n0.017\n0.075\n0.141\n\n\nResidual Standard Deviation\n\n\nSigma\n0.091\n0.001\n0.088\n0.094\n\n\n\n\n\n\n\n\n\n\n\nModel 2: Posterior Summary\n\n\nTerm\nEstimate\nStd.error\n95% CI (low)\n95% CI (high)\n\n\n\n\nLocation Submodel\n\n\nIntercept\n7.360\n0.021\n7.318\n7.402\n\n\nSecond hatched chick\n−0.019\n0.003\n−0.024\n−0.014\n\n\nScale Submodel\n\n\nIntercept (sigma)\n−2.469\n0.022\n−2.514\n−2.426\n\n\nSecond hatched chick (sigma)\n0.130\n0.027\n0.077\n0.182\n\n\nRandom effects\n\n\nNest ID\n0.049\n0.003\n0.044\n0.054\n\n\nYear\n0.100\n0.017\n0.073\n0.139\n\n\n\n\n\n\n\n\nThe location-scale model (Model 2) was the most-supported model based on AICc values (-8070.8) compared to the location-only model (-8048.8; see Model comparison tab for details).\n\n\n\nm2_draws&lt;-fit2 %&gt;%\n  tidybayes::epred_draws(newdata = expand.grid(\n    RANK = unique(dat$RANK)\n  ), re_formula = NA, dpar =TRUE) \n\n\nplot_lm2 &lt;- ggplot(m2_draws, aes(x = factor(RANK), y = .epred,fill = factor(RANK))) +\n  geom_violin(alpha = 0.5, show.legend = FALSE)+\n  stat_pointinterval(show.legend = FALSE) +\n  labs(\n    title = \"Location\",\n    x = \"Hatching order\", \n    y = \"log(SMI)\"\n  ) +\n  theme_classic() + scale_y_continuous(breaks = seq(7.2,7.5,0.05),limits = c(7.2, 7.5))\n\n\nplot_sm2 &lt;- ggplot(m2_draws, aes(x = factor(RANK), y = sigma,fill = factor(RANK))) +\ngeom_violin(alpha = 0.5, show.legend = FALSE)+\n  stat_pointinterval(show.legend = FALSE) +\n  labs(\n    title = \"Scale\",\n    x = \"Hatching order\", \n    y = \"log(Standard Deviation)\"\n  ) +\n  theme_classic() + scale_y_continuous(breaks = seq(0.07,0.12,0.01),limits = c(0.07, 0.12))\n\ncombinded_plot&lt;-(plot_lm2 + plot_sm2)& \n  theme(\n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank()\n  )\ncombinded_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Challenge: Summarized Data vs. Distributional Plots\n\n\n\nA violin plot is a powerful tool for visualizing the entire distribution of a dataset. To draw its characteristic shape, ggplot2::geom_violin() needs the full “cloud” of possible values to calculate the data’s density at different points.\nHowever, many traditional analysis functions are designed to provide summarized marginal means, not raw posterior draws. For example, when you use a function like emmeans() on a frequentist model, it calculates a single point estimate (like a mean) and a confidence interval for each group.\nThe resulting data frame is already summarized. This summary data lacks the thousands of individual data points needed to describe a distribution’s shape. geom_violin() simply cannot work with this, as it has no information about the density of the predictions, only their central tendency and range.\nThe brms Solution: The Full Posterior Distribution\nThis is where the Bayesian workflow with brms and tidybayes excels. The tidybayes::epred_draws() function is specifically designed to extract the full posterior distribution from your brms model and provide it as a tidy, long-format data frame.\nInstead of a single summary row per group, epred_draws() gives you thousands of rows—one for each posterior draw from your model. This data frame represents the entire “cloud” of possible values that geom_violin() needs.\nBy providing the complete posterior distribution in a clean, plot-ready format, the brms and tidybayes ecosystem makes creating rich, distributional visualizations like violin plots a natural and direct step in the analysis workflow.\n\n\nPoint estimates\nPosterior distribution\n\n\n\n\n#| echo: false\n\n\nplot_location_emmeans + plot_dispersion_emmeans",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Adding random effects in the location part only (model 2)</span>"
    ]
  },
  {
    "objectID": "02_Model2.html#interpretation-of-location-scale-model",
    "href": "02_Model2.html#interpretation-of-location-scale-model",
    "title": "\n3  Adding random effects in the location part only (model 2)\n",
    "section": "\n3.5 Interpretation of location-scale model :",
    "text": "3.5 Interpretation of location-scale model :\nLocation (mean) part:\n\nThere was a conclusive effect of hatching order on the mean of log(SMI), with second-hatched chicks having a lower SMI than first-hatched chicks (\\(\\beta_{[\\text{first-second}]}^{(l)}\\) = -0.018, 95% CI = [-0.024, -0.013]).In percentage terms, the SMI for second-hatched chicks is estimated to be 1.8% lower than for first-hatched chicks\n\nScale (dispersion) part:\n\nThe scale component revealed a conclusive difference in the variability of SMI between hatching orders. The second-hatched chicks exhibited greater variability in SMI compared to first-hatched chicks (\\(\\beta_{[\\text{first-second}]}^{(s)}\\) = 0.130, 95% CI = [0.077, 0.183]).This means the standard deviation of SMI for second-hatched chicks is 13.9% higher than for first-hatched chicks\n\nLocation (mean) random effects:\n\nThere is very little variation in the average body condition across different nests (\\(\\sigma_{[\\text{Nest\\_ID}]}^{(l)}\\) = 0.049, 95% CI [0.044, 0.054]), suggesting that most nests have similar average body conditions for their chicks.\nThere is also very little variation in the average body condition across different years (\\(\\sigma_{[\\text{Year}]}^{(l)}\\) = 0.092, 95% CI [0.069, 0.124]),implying generally consistent average body conditions from year to year.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Adding random effects in the location part only (model 2)</span>"
    ]
  },
  {
    "objectID": "02_Model2.html#conclusion",
    "href": "02_Model2.html#conclusion",
    "title": "\n3  Adding random effects in the location part only (model 2)\n",
    "section": "\n3.6 Conclusion",
    "text": "3.6 Conclusion\nDo first hatched chicks have a higher body mass than second-hatched chicks?\nAnswer: Answer: Yes, first-hatched chicks have a significantly higher Scaled Mass Index (SMI) than second-hatched chicks at fledgling, with a mean difference of approximately 0.018 on the log scale.\nDoes the variability in body mass differ between first and second-hatched chicks?\nAnswer: Yes, the variability in body mass (SMI) is significantly greater in second-hatched chicks compared to first-hatched chicks. This indicates that second-hatched chicks show more variation in their body condition at fledgling.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Adding random effects in the location part only (model 2)</span>"
    ]
  },
  {
    "objectID": "03_Model3.html",
    "href": "03_Model3.html",
    "title": "\n4  Double-hierarchical location-scale model (Model 3)\n",
    "section": "",
    "text": "4.1 Dataset overview\nFollowing up on the previous example analyzing the scaled mass index (SMI) at fledgling Drummond et al(2025), we now extend our approach by fitting a double-hierarchical Gaussian location-scale model. This extends Model 2 by introducing nest identity as a correlated random effect in both the location (mean) and scale (variance) components. This extension allows us to examine not only how average log(SMI) and its variability differ across nests, but also whether these two forms of nest-level variation are correlated. The main distinction between Model 2 and Model 3 lies in how they handle variation. Model 2 includes random effects only for the mean, assuming constant variance across groups. In contrast, Model 3 accounts for group-level variability by including random effects in both the mean and the variance components. This makes Model 3 particularly useful when:\nBy modeling variation explicitly, Model 3 can yield better fit and more nuanced inference, especially in ecological or evolutionary contexts where both central tendencies and dispersion carry important signals.\nWe use the same dataset as in Model 2, but extend the location-scale framework by moving from Model 2 to Model 3, a double-hierarchical location-scale model. This extension allows us to model group-level variation in both the mean (location) and the variance (scale), as well as their potential correlation.\ndat &lt;- read_csv(here(\"data\",\"SMI_Drummond_et_al_2025.csv\")) %&gt;%subset(REDUCTION == 0)\n\ndat&lt;- dat%&gt;%\n  dplyr::select(-TIME, -HATCHING_DATE,-REDUCTION,-RING) %&gt;%\n  mutate(SMI = as.numeric(SMI),# Scaled mass index\n         lnSMI=log(SMI),\n         NEST = as.factor(NEST), \n         WORKYEAR = as.factor(WORKYEAR),\n         RANK = factor(RANK, levels = c(\"1\", \"2\")))",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Double-hierarchical location-scale model (Model 3)</span>"
    ]
  },
  {
    "objectID": "03_Model3.html#dataset-overview",
    "href": "03_Model3.html#dataset-overview",
    "title": "\n4  Double-hierarchical location-scale model (Model 3)\n",
    "section": "",
    "text": "4.1.1 Question\nVariation between and within nests Do nests with generally healthier (heavier) chicks also tend to have chicks that are more similar in their body condition?",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Double-hierarchical location-scale model (Model 3)</span>"
    ]
  },
  {
    "objectID": "03_Model3.html#run-models-and-interpret-results",
    "href": "03_Model3.html#run-models-and-interpret-results",
    "title": "\n4  Double-hierarchical location-scale model (Model 3)\n",
    "section": "\n4.2 Run models and interpret results",
    "text": "4.2 Run models and interpret results\nIn this section, we compare Model 1 and Model 3 using the brms package, since glmmTMB does not support estimating correlations between random effects in the location and scale components. Model 3 - the double-hierarchical location-scale model - extends Model 2 by incorporating random effects for NEST_ID in both the mean and the variability of SMI. Crucially, it also estimates the correlation between these nest-level effects. For instance, a positive correlation would indicate that nests with higher average SMI also exhibit greater variability in SMI.\n\n\nDouble-hierarchical Location-scale model\nModel comparison\nSummary of model results\nPlot results\n\n\n\n\nm3 &lt;- bf(log(SMI) ~ 1 + RANK+ (1|q|NEST) + (1|WORKYEAR),\n         sigma~ 1 + RANK + (1|q|NEST) + (1|WORKYEAR))\nprior3&lt;-default_prior(m3,data = dat,family = gaussian())\n \nfit3 &lt;- brm(\n  m3,\n  prior= prior3,\n  data = dat,\n  family = gaussian(),\n  iter = 6000,     \n  warmup = 1000,   \n  chains = 4,  \n  cores=4,\n  backend = \"cmdstanr\",\n  control = list(\n    adapt_delta = 0.99,  \n    max_treedepth = 15  \n    ),\n  seed = 123,      \n  refresh = 500)\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: log(SMI) ~ (1 | q | NEST) + (1 | WORKYEAR) + RANK \n         sigma ~ (1 | q | NEST) + (1 | WORKYEAR) + RANK\n   Data: dat (Number of observations: 4737) \n  Draws: 4 chains, each with iter = 6000; warmup = 1000; thin = 1;\n         total post-warmup draws = 20000\n\nMultilevel Hyperparameters:\n~NEST (Number of levels: 2873) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(Intercept)                      0.05      0.00     0.04     0.05 1.00\nsd(sigma_Intercept)                0.36      0.02     0.32     0.40 1.00\ncor(Intercept,sigma_Intercept)    -0.46      0.07    -0.58    -0.33 1.00\n                               Bulk_ESS Tail_ESS\nsd(Intercept)                      3596     7826\nsd(sigma_Intercept)                4668     9783\ncor(Intercept,sigma_Intercept)     3805     7596\n\n~WORKYEAR (Number of levels: 24) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.10      0.02     0.07     0.14 1.00     5949    10071\nsd(sigma_Intercept)     0.28      0.06     0.18     0.41 1.00     8421    12541\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           7.36      0.02     7.32     7.40 1.00     3428     6959\nsigma_Intercept    -2.52      0.07    -2.65    -2.38 1.00     5030     8529\nRANK2              -0.01      0.00    -0.02    -0.01 1.00    24251    17886\nsigma_RANK2         0.13      0.03     0.07     0.19 1.00    19128    15170\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\nRemember that to interpret a coefficient (β) for a given level of RANK relative to the reference level, you must exponentiate it.\nThe multiplicative factor on SMI is \\(exp(β)\\).\nThe percentage change in SMI is \\((exp(β) - 1) * 100\\%\\).\n\n\n\n\n\nWe compare the double-hierarchical location-scale model (Model 3) with the previous location-only model (Model 1) to examine whether the added complexity of Model 3 leads to a better fit to the data.\n\nf1loo &lt;- loo::loo(fit1)\nf3loo &lt;- loo::loo(fit3)\n\n#Model comparison \nfc2 &lt;- loo::loo_compare(f1loo, f3loo)\nfc2\n#  elpd_diff se_diff\n# fit3    0.0       0.0 \n# fit1 -357.2      37.4 \n\n\n\nThe impact of including random effects in the scale component becomes clear when comparing the results of Model 1 with those of the double-hierarchical location-scale model (Model 3).\n\n\n\nm3_draws&lt;-fit3 %&gt;%\n  tidybayes::epred_draws(newdata = expand.grid(\n    RANK = unique(dat$RANK)\n  ), re_formula = NA, dpar =TRUE) \n\n\nplot_lm3 &lt;- ggplot(m3_draws, aes(x = factor(RANK), y = .epred,fill = factor(RANK))) +\n  geom_violin(alpha = 0.5, show.legend = FALSE)+\n  stat_pointinterval(show.legend = FALSE) +\n  labs(\n    title = \"Location\",\n    x = \"Hatching order\", \n    y = \"log(SMI)\"\n  ) +\n  theme_classic() + scale_y_continuous(breaks = seq(7.2,7.5,0.05),limits = c(7.2, 7.5))\n\n\nplot_sm3 &lt;- ggplot(m3_draws, aes(x = factor(RANK), y = sigma,fill = factor(RANK))) +\ngeom_violin(alpha = 0.5, show.legend = FALSE)+\n  stat_pointinterval(show.legend = FALSE) +\n  labs(\n    title = \"Scale\",\n    x = \"Hatching order\", \n    y = \"log(Standard Deviation)\"\n  ) +\n  theme_classic() + scale_y_continuous(breaks = seq(0.04,0.12,0.01),limits = c(0.04, 0.12))\n\n\n\ncorrelation_draws &lt;- fit3 %&gt;%\n  spread_draws(cor_NEST__Intercept__sigma_Intercept)\n\n\n\n\n\ncorr_plot&lt;-ggplot(data=correlation_draws,aes(x = cor_NEST__Intercept__sigma_Intercept)) +\n   stat_halfeye() + \n  labs(\n    title = \"Correlation between location and the scale intercepts in the NEST group\",\n    x = \"cor(Intercept, sigma_Intercept)\",\n    y = \"Density\"\n  ) +\n  theme_classic()\n\n\n\ncombinded_plot&lt;-((plot_lm3 + plot_sm3)/corr_plot)& \n  theme(\n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank()\n  )\ncombinded_plot\n\n\n\n\n\n\n\n\n\n\n\n4.2.1 Comparison of location-only model and location-scale model\nThe model comparison results indicate that the double-hierarchical location-scale model (Model 3) is the most supported by the data, as it has the lowest LOO (Leave-One-Out Cross-Validation) information criterion value. This suggests that this model provides the best fit to the data while accounting for both the average differences in body condition and the variability in body condition between first and second-hatched chicks.\n\n4.2.2 Interpretation of location-scale model :\nScale (dispersion) random effects:\n\nThere is notable variation in body condition within nests (\\(\\sigma_{[\\text{Nest\\_ID}]}^{(s)}\\) = 0.37, 95% CI [0.316, 0.399]). This indicates that some nests consistently produce chicks with more consistent body conditions than others.\nThe consistency of chick body conditions varies notably across work years (\\(\\sigma_{[\\text{Year}]}^{(s)}\\) = 0.277, 95% CI [0.183, 0.411]), implying some years yield chicks with more consistent body conditions than others.\n\nCorrelation between location and scale random effects:\n\nThe correlation between the nest-specific random effects for the average body condition and the variability in body condition is negative (\\(\\rho_{[\\text{Nest\\_ID}]}\\) = -0.457, 95% CI [-0.584, -0.326]). This suggests that nests with higher average body conditions tend to produce chicks with more consistent body condition values (less dispersion).",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Double-hierarchical location-scale model (Model 3)</span>"
    ]
  },
  {
    "objectID": "03_Model3.html#conclusion",
    "href": "03_Model3.html#conclusion",
    "title": "\n4  Double-hierarchical location-scale model (Model 3)\n",
    "section": "\n4.3 Conclusion",
    "text": "4.3 Conclusion\nDo nests with generally healthier (heavier) chicks also tend to have chicks that are more similar in their body condition?\nAnswer: Yes, there’s a tendency for nests with higher average chick body condition to also exhibit greater consistency among their chicks’ body conditions. This is supported by a negative correlation between nest-specific average body condition and its variability.\n\n\n\n\n\n\nNote\n\n\n\nModeling Correlated Random Effects with brms vs. glmmTMB\nThis specific hierarchical model can be fit using brms but not with glmmTMB due to a key difference in their capabilities.\nThe model has two parts: one predicting the mean (mu) of the outcome and another predicting its variability (sigma). For each grouping factor (e.g., NEST), the model estimates a random effect for both the mean and the variability.\nThe core issue is that glmmTMB cannot estimate the correlation between these two different random effects. It is forced to assume that a group’s effect on the mean is completely unrelated to its effect on the variability. brms, being a more flexible Bayesian tool, can estimate this relationship directly from the data.\nTherefore, if your hypothesis involves a potential link between the random effects for different parts of your model (e.g., “do nests with higher average SMI also have less variable SMI?”), you must use a package like brms that can model this covariance.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Double-hierarchical location-scale model (Model 3)</span>"
    ]
  },
  {
    "objectID": "04_BeyondGaussian1.html",
    "href": "04_BeyondGaussian1.html",
    "title": "\n5  Beyond Gaussian 1\n",
    "section": "",
    "text": "5.0.1 Questions\nThis dataset comes from Mizuno and Soma (2023), which investigates the visual preferences of estrildid finches. The study measured how often birds gazed at different visual stimuli (white dots vs. white stripes) under two conditions: food-deprived and food-supplied. The research originally tested the sensory bias hypothesis, which suggests that a pre-existing preference for whitish, round objects (like seeds) may have influenced the evolution of plumage patterns.\nHere, to keep things simple, we use only a subset of the data related to the white dot pattern stimulus. We analyse how gaze frequency toward dot stimuli differs between the two conditions (food-deprived vs. food-supplied), using a location-scale model with a negative binomial distribution.\nAlthough this model includes two random effects (individual and species), it follows the same basic structure as Model 2 - the only difference is that we use a non-Gaussian distribution (negative binomial) instead of a Gaussian one.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Beyond Gaussian 1</span>"
    ]
  },
  {
    "objectID": "04_BeyondGaussian1.html#visualise-the-datasets",
    "href": "04_BeyondGaussian1.html#visualise-the-datasets",
    "title": "\n5  Beyond Gaussian 1\n",
    "section": "\n5.1 Visualise the datasets",
    "text": "5.1 Visualise the datasets\n\n\nCondition\nSpecies\n\n\n\nFirst, we visualise the data to understand how gaze frequency varies by condition and species.\n\nset.seed(42) \n\n# load the dataset ----\ndat_pref &lt;- read.csv(here(\"data\", \"AM_preference.csv\"), header = TRUE)\n\ndat_pref &lt;- dat_pref %&gt;%\n  dplyr::select(-stripe, -full, -subset) %&gt;%\n  rename(frequency = dot) %&gt;%\n  mutate(species = phylo, across(c(condition, sex), as.factor))\n  \nggplot(dat_pref, aes(x = condition,\n                     y = frequency)) +\n    geom_violin(color = \"#8B8B83\", fill = \"white\",\n              width = 1.2, alpha = 0.3) +\n  geom_jitter(aes(shape = sex, color = sex),\n              size = 3, alpha = 0.8,\n              width = 0.15, height = 0) +\n  labs(\n    title = \"Total frequency of gazes towards dot patterns\",\n    x = \"Condition\",\n    y = \"Total frequency of gazes (1hr)\"\n  ) +\n  scale_shape_manual(values = c(\"M\" = 17, \"F\" = 16),\n                     labels = c(\"M\" = \"Male\", \"F\" = \"Female\")) +\n  scale_color_manual(values = c(\"M\" = \"#009ACD\", \"F\" = \"#FF4D4D\"),\n                     labels = c(\"M\" = \"Male\", \"F\" = \"Female\")) +\n  theme_classic(base_size = 16) +\n  theme(\n    axis.text = element_text(color = \"#6E7B8B\", size = 14),\n    axis.title = element_text(color = \"#6E7B8B\", size = 14),\n    legend.title = element_text(color = \"#6E7B8B\"),\n    legend.text = element_text(color = \"#6E7B8B\"),\n    legend.position = \"right\",\n  axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nThe plot breaks down the same gaze data by species. Each point is a bird’s gaze frequency under a condition. Colored lines represent species-specific means across conditions, helping to visualise - how different species vary in their average gazing behaviour, whether species respond differently to food deprivation. Faceting by species can help compare species-level patterns.\n\nggplot(dat_pref, aes(x = condition, y = frequency)) +\n  geom_point(alpha = 0.5) +\n  labs(\n    title = \"Total frequency of gazes towards dot patterns by species\",\n    x = \"Condition\",\n    y = \"Total frequency of gazes (1hr)\"\n  ) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = species, color = species)) +\n  theme_classic() +\n  facet_wrap(~ species)",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Beyond Gaussian 1</span>"
    ]
  },
  {
    "objectID": "04_BeyondGaussian1.html#run-models-and-interpret-results",
    "href": "04_BeyondGaussian1.html#run-models-and-interpret-results",
    "title": "\n5  Beyond Gaussian 1\n",
    "section": "\n5.2 Run models and interpret results",
    "text": "5.2 Run models and interpret results\nWe fit and compare two types of models to analyse gaze frequency toward white dot patterns using the glmmTMB package:\n\nLocation-only model: Estimates how the mean gaze frequency varies between the two experimental conditions (food-deprived vs. food-supplied).\nLocation-scale model: Estimates how both the mean and the variability (dispersion) of gaze frequency vary between conditions.\n\nThese models help us examine whether internal states such as hunger (represented by food deprivation) influence not only the average number of gazes, but also the species/individual variability in gaze behaviour. Such variability may reflect differences in exploratory tendencies among species/individuals. Please note that in this section, species are not modelled with phylogenetic relatedness. For an example of how to fit a location–scale phylogenetic regression model, see Section Model selection (example 2).\n\n5.2.1 Model fitting\n\n\nLocation model\nResidual diagnostics\nLocation-scale model\nModel comparison\nSummary of model results\nBonus 1 - Conway-Maxwell-Poisson (CMP) model\nBonus 2 - brms\nResult figures\n\n\n\nWe use negative binomial model here, as the response variable frequency is a count of gazes and may exhibit overdispersion. In The glmmTMB package, we can specify the family as nbinom2 for negative binomial distribution with a log link function.\n\n# location model ----\n## the mean (location) of gaze frequency with condition and sex as fixed effects; assumes constant variance.\n\nmodel_0 &lt;- glmmTMB(\n  frequency ~ 1 +  condition + sex + (1 | species) + (1 | id), # location part (mean)\n  data = dat_pref,\n  family = nbinom2(link = \"log\")\n)\n \nsummary(model_0)\n\n Family: nbinom2  ( log )\nFormula:          frequency ~ 1 + condition + sex + (1 | species) + (1 | id)\nData: dat_pref\n\n     AIC      BIC   logLik deviance df.resid \n  2176.3   2195.8  -1082.2   2164.3      184 \n\nRandom effects:\n\nConditional model:\n Groups  Name        Variance Std.Dev.\n species (Intercept) 0.3725   0.6103  \n id      (Intercept) 0.1353   0.3679  \nNumber of obs: 190, groups:  species, 12; id, 95\n\nDispersion parameter for nbinom2 family (): 1.72 \n\nConditional model:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        4.84086    0.22151  21.854  &lt; 2e-16 ***\nconditionsupplied -0.92168    0.12080  -7.630 2.36e-14 ***\nsexM              -0.06673    0.14422  -0.463    0.644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(model_0)\n\n                                 2.5 %     97.5 %   Estimate\n(Intercept)                  4.4067149  5.2750101  4.8408625\nconditionsupplied           -1.1584490 -0.6849075 -0.9216782\nsexM                        -0.3493977  0.2159315 -0.0667331\nStd.Dev.(Intercept)|species  0.3467243  1.0743326  0.6103255\nStd.Dev.(Intercept)|id       0.2105812  0.6427136  0.3678905\n\n\n\n\nFor Gaussian models, we can use the Q-Q plot to show whether the residuals fall along a straight line. However, for non-Gaussian models, we can use the DHARMa package to check the residuals. The DHARMa package provides a set of diagnostic tools for generalized linear mixed models (GLMMs) and allows us to simulate residuals and check for overdispersion, outliers, and uniformity.\n\n# location-only model (NB) ----\n# main diagnostic plots\nmodel0_res &lt;- simulateResiduals(model_0, plot = TRUE, seed = 42)\n\n\n\n\n\n\n# formal test for over/underdispersion\ntestDispersion(model0_res) \n\n\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.45606, p-value = 0.456\nalternative hypothesis: two.sided\n\n\nThe DHARMa diagnostics indicate no issues with overdispersion or outliers (Dispersion test: p = 0.456; Outlier test: p = 0.88). However, the KS test for uniformity is significant (p = 2e-05), suggesting deviation from the expected residual distribution. Additionally, residuals show non-uniform patterns across levels of the categorical predictor (catPred), and the Levene’s test indicates heterogeneity of variance among groups. These results suggest that the model may not fully capture the structure associated with the categorical predictor.\n\n\nThe result fromDHARMa diagnostics suggests that the location-only model may not fully capture the structure of the data. Therefore, we can fit a location-scale model to account for both the mean and variance of the response variable.\n\n# location-scale model ---- \n# both the mean (location) and the variance (scale) as functions of condition and sex.\n\nmodel_1 &lt;- glmmTMB(\n  frequency ~ 1 + condition + sex + (1 | species) + (1 | id),\n   dispformula = ~ condition + sex,     \n  data = dat_pref,                          \n  family = nbinom2(link = \"log\")\n)\nsummary(model_1)\n\n Family: nbinom2  ( log )\nFormula:          frequency ~ 1 + condition + sex + (1 | species) + (1 | id)\nDispersion:                 ~condition + sex\nData: dat_pref\n\n     AIC      BIC   logLik deviance df.resid \n    2172     2198    -1078     2156      182 \n\nRandom effects:\n\nConditional model:\n Groups  Name        Variance Std.Dev.\n species (Intercept) 0.3047   0.5520  \n id      (Intercept) 0.1173   0.3425  \nNumber of obs: 190, groups:  species, 12; id, 95\n\nConditional model:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         4.8737     0.2042  23.868  &lt; 2e-16 ***\nconditionsupplied  -0.8452     0.1206  -7.010 2.38e-12 ***\nsexM               -0.1040     0.1370  -0.759    0.448    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n                  Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         0.8394     0.2597   3.232  0.00123 **\nconditionsupplied  -0.6616     0.2480  -2.667  0.00765 **\nsexM                0.1320     0.2486   0.531  0.59549   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(model_1)\n\n                                      2.5 %     97.5 %   Estimate\ncond.(Intercept)                  4.4734822  5.2739084  4.8736953\ncond.conditionsupplied           -1.0814991 -0.6088973 -0.8451982\ncond.sexM                        -0.3725499  0.1646364 -0.1039568\ndisp.(Intercept)                  0.3303327  1.3485194  0.8394261\ndisp.conditionsupplied           -1.1476836 -0.1754389 -0.6615612\ndisp.sexM                        -0.3552045  0.6191168  0.1319561\ncond.Std.Dev.(Intercept)|species  0.3079456  0.9894436  0.5519916\ncond.Std.Dev.(Intercept)|id       0.1719628  0.6820707  0.3424774\n\n\n\n\nLet’s compare the two models to see if the location-scale model provides a better fit to the data than the location-only model.\n\n# compare models ----\nmodel.sel(model_0, model_1)\n\n\n  \n\n\n\n\n\nYou can quickly check the results of the location-only model (model 0) and the location-scale model (model 1) below.\n\n\n\n\n\n\n\nLocation-only model\n\n\nTerm\nEstimate\nStd. Error\n95% CI (low)\n95% CI (high)\n\n\n\n\n(Intercept)\n4.841\n0.222\n4.407\n5.275\n\n\nconditionsupplied\n−0.922\n0.121\n−1.158\n−0.685\n\n\nsexM\n−0.067\n0.144\n−0.349\n0.216\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation-scale model (location part)\n\n\nTerm\nEstimate\nStd. Error\n95% CI (low)\n95% CI (high)\n\n\n\n\n(Intercept)\n4.874\n0.204\n4.473\n5.274\n\n\nconditionsupplied\n−0.845\n0.121\n−1.081\n−0.609\n\n\nsexM\n−0.104\n0.137\n−0.373\n0.165\n\n\n\n\n\n\n\n\n\n\n\nLocation-scale model (dispersion part)\n\n\nTerm\nEstimate\nStd. Error\n95% CI (low)\n95% CI (high)\n\n\n\n\n(Intercept)\n0.839\n0.260\n0.330\n1.349\n\n\nconditionsupplied\n−0.662\n0.248\n−1.148\n−0.175\n\n\nsexM\n0.132\n0.249\n−0.355\n0.619\n\n\n\n\n\n\n\n\nThe Conway-Maxwell-Poisson (CMP) model is a flexible count data model that can handle overdispersion and underdispersion.\n\n# CMP model ----\n\nmodel_2 &lt;- glmmTMB(\n  frequency ~ 1 + condition + sex + (1 | species) + (1 | id),\n  dispformula = ~ condition + sex, \n  data = dat_pref,\n  family = compois(link = \"log\")\n)\nsummary(model_2)\n\nconfint(model_2)\n\nmodel.sel(model_0, model_1, model_2)\n\n\n\n Family: compois  ( log )\nFormula:          frequency ~ 1 + condition + sex + (1 | species) + (1 | id)\nDispersion:                 ~condition + sex\nData: dat_pref\n\n     AIC      BIC   logLik deviance df.resid \n  2158.4   2184.4  -1071.2   2142.4      182 \n\nRandom effects:\n\nConditional model:\n Groups  Name        Variance Std.Dev.\n species (Intercept) 0.246    0.4960  \n id      (Intercept) 0.151    0.3886  \nNumber of obs: 190, groups:  species, 12; id, 95\n\nConditional model:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         4.8614     0.1857  26.178  &lt; 2e-16 ***\nconditionsupplied  -0.7823     0.1044  -7.495 6.62e-14 ***\nsexM               -0.1099     0.1257  -0.874    0.382    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         4.0338     0.4457   9.050   &lt;2e-16 ***\nconditionsupplied   1.1965     0.7681   1.558    0.119    \nsexM               -0.1428     0.5530  -0.258    0.796    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n                                      2.5 %     97.5 %   Estimate\ncond.(Intercept)                  4.4974266  5.2253653  4.8613959\ncond.conditionsupplied           -0.9868149 -0.5776988 -0.7822569\ncond.sexM                        -0.3562620  0.1364832 -0.1098894\ndisp.(Intercept)                  3.1602735  4.9074247  4.0338491\ndisp.conditionsupplied           -0.3089514  2.7019951  1.1965218\ndisp.sexM                        -1.2266046  0.9409510 -0.1428268\ncond.Std.Dev.(Intercept)|species  0.2691415  0.9139714  0.4959714\ncond.Std.Dev.(Intercept)|id       0.2503174  0.6033250  0.3886165\n\n\n\n  \n\n\n\nHere, we compared three generalised linear mixed models. model_0 was a Negative Binomial (NB) model with a single, overall estimated dispersion parameter (i.e., not modelled as a function of predictors). model_1 extended this by modelling the NB dispersion parameter as a function of condition and sex. model_2 further used a Conway-Maxwell-Poisson (CMP) distribution, which can accommodate both under- and over-dispersion, with its dispersion parameter similarly modelled by condition and sex.\nModel comparison based on AICc strongly favored the location-scale CMP model (model_2: AICc = 2159.2, model weight = 0.999), with the location-scale NB model (model_1: AICc = 2172.8) and the NB model with a single dispersion parameter (model_0: AICc = 2176.8) performing substantially worse.\nIn all three models, food-supplied birds showed significantly lower gaze frequencies toward dot stimuli. There was no evidence of sex differences in the location part in any model.\nRegarding the dispersion parameter, in the location-scale NB model (model_1), the dispersion part revealed a significant change in residual variance in the food-supplied condition (\\(\\beta_{[\\text{conditionSupplied}]}^{(l)}\\) = –0.662). Given the parameterisation of the Negative Binomial distribution (where a smaller dispersion parameter \\(\\theta\\) indicates greater overdispersion), this suggests an increase in overdispersion when food was supplied, implying less consistent behaviour across individuals. However, this effect was not statistically significant in the location-scale CMP model (model_2), where the corresponding estimate was 1.20 (p = 0.12). In the CMP model, a positive estimate for the dispersion parameter \\(\\nu\\) would imply a decrease in overdispersion (i.e., more consistent behavior) if it were significant.\nRandom effects in all models consistently showed greater variance among species than among id within species. For example, in the location-scale CMP model, species-level variance was 0.246, while individual-level variance was 0.151.\n\n5.2.2 Comparing Negative Binomial and CMP Models\nBelow, we compare the Negative Binomial (NB) and Conway-Maxwell-Poisson (CMP) models to assess which better fits the data. Model selection is based on AICc and residual diagnostics.\n\n\n\nModel comparison of location-only and location-scale models in negative Binomial (NB) and Conway-Maxwell-Poisson (CMP) distributions.\n\n\n\nAll models passed the DHARMa dispersion and outlier tests, indicating appropriate handling of overall variance and absence of extreme observations. However, the Kolmogorov–Smirnov (KS) test consistently revealed significant deviations from the expected uniform distribution of residuals in all three cases, suggesting remaining misfits in distributional shape. The location-scale CMP model performed best in terms of within-group residual uniformity, showing no significant deviation in any predictor level. In contrast, both the location-only NB model and the location-scale NB model exhibited within-group deviations in some categories. These results collectively suggest that while none of the models perfectly capture the residual structure, the location-scale CMP model may offer the best overall fit among the candidates for the observed data characteristics.\n\n\n\nYou can also fit the location-scale model using the brms package. The difference is that in brms, you can specify scale parts using shape. Below, we show how to fit the same location-scale negative binomial model as above using brms.\n\nformula1 &lt;- bf(\n  frequency ~ 1 + condition + sex + (1 | species) + (1 | id),\n  shape ~ 1 + condition + sex\n)\n\nprior1 &lt;- default_prior(formula1, \n                        data = dat_pref, \n                        family = negbinomial(link = \"log\", link_shape = \"log\")\n)\n\nsystem.time(model_nb_brms &lt;- brm(formula1, \n            data = dat_pref, \n            prior = prior1,\n            chains = 2, \n            iter = 5000, \n            warmup = 3000,\n            thin = 1,\n            family = negbinomial(link = \"log\", link_shape = \"log\"),\n            # control = list(adapt_delta = 0.95)\n)\n)\nsummary(model_nb_brms)\n\n\n\n Family: negbinomial \n  Links: mu = log; shape = log \nFormula: frequency ~ condition + sex + (1 | species) + (1 | id) \n         shape ~ condition + sex\n   Data: dat_pref (Number of observations: 190) \n  Draws: 2 chains, each with iter = 5000; warmup = 3000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 95) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.38      0.13     0.08     0.60 1.00      482      563\n\n~species (Number of levels: 12) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.69      0.23     0.36     1.24 1.00      978     1816\n\nRegression Coefficients:\n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                   4.85      0.25     4.35     5.34 1.00     1044\nshape_Intercept             0.87      0.28     0.36     1.45 1.00      966\nconditionsupplied          -0.84      0.12    -1.09    -0.60 1.00     4843\nsexM                       -0.11      0.14    -0.40     0.17 1.00     3632\nshape_conditionsupplied    -0.70      0.28    -1.29    -0.19 1.00     2044\nshape_sexM                  0.14      0.25    -0.35     0.61 1.00     4208\n                        Tail_ESS\nIntercept                   1637\nshape_Intercept             1938\nconditionsupplied           3049\nsexM                        3220\nshape_conditionsupplied     1777\nshape_sexM                  2673\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\nPosterior predictive checks indicated that the negative binomial location–scale model adequately captured the central tendency of gaze frequencies. However, the model slightly underestimated the heaviness of the right tail, suggesting that a small number of extreme observations were not fully reproduced.\nNote that we can also conduct the CMP model in brms using the family = brmsfamily(\"com_poisson\") family, but it is still at an experimental stage and may not work well, so we recommend the glmmTMB package for CMP models for now…\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nLocation part: Points and error bars indicate the estimated log(mean frequency) for each combination of feeding condition and sex. For glmmTMB, values are estimated marginal means (on the log scale) with 95% confidence intervals. For brms, values are posterior medians with 95% credible intervals. In addition, the brms panels include violin plots, which visualise the full posterior distributions of the estimates. Faint dots represent model-based predictions of log(mean frequency) for individual observations (i.e. fitted values including random effects), shown to illustrate how the model maps observed data points onto the underlying mean structure.\nScale part: Points and error bars show the estimated log(theta), the dispersion (shape) parameter of the negative binomial distribution, for each condition–sex cell. Because theta is a distributional parameter rather than an observed variable, there are no fitted or raw data points that can be plotted at the observation level. In the brms panels, violin plots again illustrate the posterior distributions for log(theta), while points and intervals indicate the posterior median and 95% credible interval.\n\n\n\n\n\n5.2.3 Comparison of location-only model and location-scale model\n\nModeling dispersion improves model fit and better captures the structure of the data.\nLocation-scale model had a lower AICc (2172.8) than location-only model (2176.8), indicating better model fit. It also had a higher model weight (0.877 vs. 0.123), suggesting stronger support for the location-scale model.\n\n5.2.4 Interpretation of location-scale model :\n\n5.2.4.1 How to back-transflrm the log-link scale to natural scale?\n\nThe location part uses a log link, so estimates are on the log(mean frequency) scale. To obtain the expected mean frequency on the natural scale, simply exponentiate the estimate.\nThe dispersion part also uses a log link, so estimates are on the log(theta) scale. To get theta on the natural scale, exponentiate the estimate.\n\n5.2.4.2 Biological meanings:\nLocation (mean) part:\n\nBirds gazed significantly less at dots when food was supplied (\\(\\beta^{(l)}_{\\text{deprived–supplied}} = -0.85\\), 95% CI -1.08, -0.61), corresponding to a rate ratio of \\(\\exp(\\beta^{(l)}) = 0.43\\) (i.e., mean frequency reduced by 57.3%; CI −66.0% to −45.7%).\nThere is no significant sex difference in mean gaze frequency (\\(\\beta_{[\\text{sex-male}]}^{(l)}\\) = -0.104, 95% CI [-0.37, 0.16]$).\n\nScale (dispersion) part:\n\nThe supplied condition significantly reduced the precision parameter (\\(\\beta^{(s)}_{\\text{deprived–supplied}}=-0.66\\), 95% CI -1.15, -0.18), giving a \\(\\theta\\)-ratio of \\(\\exp(\\beta^{(s)})=0.52\\) (i.e., 48.3% lower precision; CI −68.3% to −16.5%). Because lower \\(\\theta\\) implies more scatter than the Poisson expectation, this means individual gaze behaviour was more variable when food was supplied.\nThere was no significant sex difference in the variability of gaze frequency (\\(\\beta^{(s)}_{\\text{sex-male}}=0.13\\), 95% CI -0.36, 0.62).\n\nLocation (mean) random effects:\n\nSpecies-level variation (\\(sd^{(l)}_{\\text{species}}=0.55\\), 95% CI 0.31, 0.99) exceeded within-species individual variation.\n\nLocation (dispersion) random effects:\n\nIndividual-level variation (\\(sd^{(l)}_{\\text{id}}=0.34\\), 95% CI 0.17, 0.68) was present but smaller than species-level differences.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Beyond Gaussian 1</span>"
    ]
  },
  {
    "objectID": "04_BeyondGaussian1.html#conclusion",
    "href": "04_BeyondGaussian1.html#conclusion",
    "title": "\n5  Beyond Gaussian 1\n",
    "section": "\n5.3 Conclusion",
    "text": "5.3 Conclusion\nQ1: Do birds gaze at dot patterns more (or less) when food-deprived compared to food-supplied?\nAnswer: Yes. The average gaze frequency is lower when food is supplied.\nQ2: Does the variability in gaze responses differ between conditions?\nAnswer: Yes. Birds showed more consistent gaze responses when food was deprived, and more varied responses when food was available.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Beyond Gaussian 1</span>"
    ]
  },
  {
    "objectID": "05_BeyondGaussian2.html",
    "href": "05_BeyondGaussian2.html",
    "title": "\n6  Beyond Gaussian 2\n",
    "section": "",
    "text": "6.1 Questions\nThis dataset comes from Lundgren et al. (2022), which examined the activity and impact of feral donkeys (Equus africanus asinus) between wetlands with and without cougar (Puma concolor) predation.\nFor this example, we will focus on the effect of cougar predation on the percentage of ground trampled by feral donkeys.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond Gaussian 2</span>"
    ]
  },
  {
    "objectID": "05_BeyondGaussian2.html#questions",
    "href": "05_BeyondGaussian2.html#questions",
    "title": "\n6  Beyond Gaussian 2\n",
    "section": "",
    "text": "Does cougar predation affect the percentage of ground trampled by feral donkeys?\nDoes cougar predation affect the variability in the percentage of ground trampled by feral donkeys?\n\n\n6.1.1 Variables included\n\n\n\n\n\n\n\n\ncover: Percentage of ground trampled by feral donkeys\n\nSite: Name of the site where the data was collected\n\nAccess_point: Access point to the sites\n\nif_kill: Whether a cougar kill (dead donkey) was found at the site (1 = yes, 0 = no)",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond Gaussian 2</span>"
    ]
  },
  {
    "objectID": "05_BeyondGaussian2.html#visualise-the-dataset",
    "href": "05_BeyondGaussian2.html#visualise-the-dataset",
    "title": "\n6  Beyond Gaussian 2\n",
    "section": "\n6.2 Visualise the dataset",
    "text": "6.2 Visualise the dataset\nViolin plots illustrate the overall density distribution of percentage of trampled Bare Ground for both areas with (Yes) and without (No) donkey kills Each individual empty circle represents the trampled bare ground cover of a single observation. The plot visually highlights individual variability and allows for the comparison of both central tendency (indicated by the black solid line, representing the mean) and spread of trampled bare ground cover between areas where burro kills are absent versus present.\n\ndat &lt;- read.csv(here(\"data\",\"Lundgren_Cougar_Burro_Trophic_Cascade_Trampled_BareGround.csv\"))\n\ndat &lt;- dat %&gt;%\n  dplyr::select(Site, Pool, if_kill, cover) %&gt;%\n  mutate(Site=as.factor(Site),\n         if_kill=as.factor(if_kill),\n         cover=as.numeric(cover)\n  )\n\nstr(dat)\n\n'data.frame':   115 obs. of  4 variables:\n $ Site   : Factor w/ 13 levels \"Anvil Spring\",..: 1 1 1 1 1 1 1 1 2 2 ...\n $ Pool   : chr  \"Anvil Spring\" \"Anvil Spring\" \"Anvil Spring\" \"Anvil Spring\" ...\n $ if_kill: Factor w/ 2 levels \"burro kills absent\",..: 1 1 1 1 1 1 1 1 2 2 ...\n $ cover  : num  1 1 1 1 1 1 1 1 1 0.6 ...\n\nggplot(dat, aes(x = if_kill, y = cover, fill = if_kill)) +\n  geom_violin(\n    aes(fill = if_kill), # Fill violins based on 'if_kill'\n    color = \"#8B8B83\", # Outline color for violins\n    width = 0.8,\n    alpha = 0.3,\n    position = position_dodge(width = 0.7)\n  ) +\n  geom_jitter(\n    aes(color = if_kill), # Color jittered points based on 'if_kill'\n    size = 3,\n    alpha = 0.4,\n    shape = 1, # Open circles for jittered points\n    position = position_jitterdodge(dodge.width = 0.5, jitter.width = 0.15)\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"crossbar\",\n    width = 0.1,\n    color = \"black\", # Black crossbar for mean\n    linewidth = 0.5,\n    position = position_dodge(width = 0.7)\n  ) +\n  labs(\n    title = \"Donkey Trampling by Cougar Kill Presence\",\n    x = \"Cougar Kill Presence\",\n    y = \"Proportion Trampled Bare Ground\"\n  ) +\n  scale_fill_manual(\n    values = c(\"burro kills absent\" = \"cornflowerblue\", \"burro kills present\" = \"firebrick\")\n  ) +\n  scale_color_manual( # Add scale_color_manual for jitter points\n    values = c(\"burro kills absent\" = \"cornflowerblue\", \"burro kills present\" = \"firebrick\")\n  ) +\n  scale_x_discrete(\n    labels = c(\"burro kills absent\" = \"No\", \"burro kills present\" = \"Yes\"),\n    expand = expansion(add = 0.5)\n  ) +\n  theme_classic(base_size = 16) +\n  theme(\n    axis.text = element_text(color = \"#6E7B8B\", size = 14),\n    axis.title = element_text(color = \"#6E7B8B\", size = 14),\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 0, hjust = 0.5)\n  )",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond Gaussian 2</span>"
    ]
  },
  {
    "objectID": "05_BeyondGaussian2.html#run-models-and-interpret-results",
    "href": "05_BeyondGaussian2.html#run-models-and-interpret-results",
    "title": "\n6  Beyond Gaussian 2\n",
    "section": "\n6.3 Run models and interpret results",
    "text": "6.3 Run models and interpret results\nWe will fit two zero-one inflated beta regression models to the data using the brms package. We’ll use the brms package because the glmmTMB (frequentist) approach does not allow for values of ‘1’, while the brms package allows us to fit zero-one inflation models to account for these boundary values. All models incorporate Pool as a random effect to account for non-independence of observations collected at the same wetland access point\nThe models are as follows:\n\nLocation-Only Model: This model will estimate the average difference in the percentage of trampled bare ground between areas where “burro kills are absent” and where “burro kills are present.”\nLocation-Scale Model: This more comprehensive model will estimate both the average difference (location) and any differences in the variability (scale) of the percentage of trampled bare ground between areas with and without burro kills.\n\n\n6.3.1 Model fitting\n\n\nLocation-only model\nLocation-scale model\nModel Comparison\nPlot results\n\n\n\n\n\n\n\n\n\nzoi ~ if_kill (Zero-One Inflation; zoi): This part of the model estimates how if_kill (cougar kill presence) influences the probability that an observation is exactly 0% or exactly 100% trampled bare ground. If if_kill increases the zoi probability, it means that areas with burro kills are more likely to have either no trampled ground or completely trampled ground, compared to areas without burro kills.\ncoi ~ if_kill (Conditional One-Inflation): This is where we distinguish between the 0% and 100% boundaries. If an observation is exactly at a boundary (0% or 100% trampled bare ground), the coi part of the model estimates how if_kill influences the probability that this boundary observation is 100% (fully trampled) rather than 0% (no trampled ground). So, if coi increases with if_kill, it suggests that if a site has exact boundary values, the presence of burro kills makes it more likely for that site to be completely trampled (100%) rather than completely untrampled (0%).\n\n\n\n\nm0&lt;-bf(cover ~ if_kill + (1|Pool),\n       zoi~  if_kill, \n       coi~  if_kill) \nprior1&lt;-default_prior(m0, family=zero_one_inflated_beta(), data=dat)\n\n# Since this model is time-consuming, reload without running it:\nrerun &lt;- F\nif(rerun){\n  \n  fit0 &lt;- brm(\n   m0,\n   data = dat,\n   family = zero_one_inflated_beta(),\n   prior = prior1,\n   iter = 6000,\n   warmup = 1000,\n   chains = 2,  cores=2,\n   control = list(\n     adapt_delta = 0.99,\n     max_treedepth = 15\n   ),\n   seed = 123,\n   refresh = 500\n  )\n  saveRDS(fit0, file = here(\"Rdata\", \"fit0_BETA_Burros.rds\"))\n  \n}else{\n  fit0 &lt;- readRDS(here(\"Rdata\", \"fit0_BETA_Burros.rds\"))\n}\n\nsummary(fit0)\n\n\n\n Family: zero_one_inflated_beta \n  Links: mu = logit; zoi = logit; coi = logit \nFormula: cover ~ if_kill + (1 | Pool) \n         zoi ~ if_kill\n         coi ~ if_kill\n   Data: dat (Number of observations: 115) \n  Draws: 2 chains, each with iter = 6000; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nMultilevel Hyperparameters:\n~Pool (Number of levels: 16) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.42      0.25     0.03     0.98 1.00     2136     2656\n\nRegression Coefficients:\n                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                        0.70      0.41    -0.11     1.52 1.00     6799\nzoi_Intercept                    0.72      0.33     0.10     1.37 1.00    15641\ncoi_Intercept                    6.31      2.97     2.51    13.90 1.00     4844\nif_killburrokillspresent        -1.02      0.46    -1.95    -0.11 1.00     7279\nzoi_if_killburrokillspresent    -1.70      0.42    -2.53    -0.89 1.00    15364\ncoi_if_killburrokillspresent    -4.24      3.06   -11.91    -0.07 1.00     4529\n                             Tail_ESS\nIntercept                        6130\nzoi_Intercept                    7396\ncoi_Intercept                    3104\nif_killburrokillspresent         6371\nzoi_if_killburrokillspresent     7048\ncoi_if_killburrokillspresent     3118\n\nFurther Distributional Parameters:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi     2.11      0.35     1.51     2.87 1.00     6161     7129\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\nphi (Precision/Scale Parameter): This part of the model estimates how if_kill (cougar kill presence) influences the variability of the percentage of trampled bare ground for observations that fall between 0% and 100% (i.e., not exactly at the boundaries). A higher phi value indicates lower variability (data points are more tightly clustered around the mean). A lower phi value indicates higher variability (data points are more spread out).\n\n\n\n\nm1&lt;-bf(cover ~ if_kill + (1|Pool),\n       zoi ~ if_kill, \n       coi ~ if_kill,\n       phi ~ if_kill) \nprior2&lt;-default_prior(m1, family=zero_one_inflated_beta(), data=dat)\n\nrerun &lt;- F\nif(rerun){\n  fit1 &lt;- brm(\n   m1,\n   data = dat,\n   family = zero_one_inflated_beta(),\n   prior = prior2,\n   iter = 6000,\n   warmup = 1000,\n   chains = 2,  cores=2,\n   control = list(\n     adapt_delta = 0.99,\n     max_treedepth = 15\n   ),\n   seed = 123,      #\n   refresh = 500    #\n  )\n  saveRDS(fit1, file = here(\"Rdata\", \"fit1_BETA_Burros.rds\"))\n}else{\n  fit1 &lt;- readRDS(here(\"Rdata\", \"fit1_BETA_Burros.rds\"))\n}\n\nsummary(fit1)\n\n\n\n Family: zero_one_inflated_beta \n  Links: mu = logit; zoi = logit; coi = logit; phi = log \nFormula: cover ~ if_kill + (1 | Pool) \n         zoi ~ if_kill\n         coi ~ if_kill\n         phi ~ if_kill\n   Data: dat (Number of observations: 115) \n  Draws: 2 chains, each with iter = 6000; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nMultilevel Hyperparameters:\n~Pool (Number of levels: 16) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.63      0.26     0.13     1.19 1.00     2237     2163\n\nRegression Coefficients:\n                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                        0.92      0.46     0.01     1.85 1.00     4499\nphi_Intercept                    1.73      0.49     0.72     2.61 1.00     3574\nzoi_Intercept                    0.72      0.33     0.10     1.39 1.00    12262\ncoi_Intercept                    6.28      2.95     2.61    14.08 1.00     4100\nif_killburrokillspresent        -1.22      0.52    -2.27    -0.18 1.00     4920\nphi_if_killburrokillspresent    -1.07      0.51    -2.01    -0.04 1.00     4334\nzoi_if_killburrokillspresent    -1.71      0.42    -2.55    -0.89 1.00    11924\ncoi_if_killburrokillspresent    -4.20      3.05   -12.07    -0.08 1.00     3953\n                             Tail_ESS\nIntercept                        5041\nphi_Intercept                    5090\nzoi_Intercept                    7049\ncoi_Intercept                    2701\nif_killburrokillspresent         5730\nphi_if_killburrokillspresent     5570\nzoi_if_killburrokillspresent     6844\ncoi_if_killburrokillspresent     2698\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\nInterpreting Model Outputs\n\n\n\nModel outputs for components like mean (mu), zero-inflation (zoi), and one-inflation (coi) are often on the logit scale, while outputs for precision (phi) are on the log scale. These scales are not directly interpretable, so we must convert them back.\nThe interpretation method changes depending on whether you’re looking at an intercept (a baseline value) or the effect of a predictor (a coefficient).\n\n\nPart 1: Interpreting Intercepts (Baseline Values)\nPart 2: Interpreting Predictor Coefficients (Effects)\n\n\n\nAn intercept gives you the starting point for your reference group.\n\n\nFor Probabilities (Logit Scale): To find the baseline probability, apply the inverse-logit function to the intercept.\n\n\nFormula: \\[\n  \\text{Probability} = \\frac{1}{1 + e^{-\\text{Estimate}}}\n  \\]\n\n\nExample (zoi_Intercept = 0.72): The probability of an exact zero in the baseline group is \\(1 / (1 + e^{-0.72}) \\approx 0.673\\), or 67.3%.\n\n\n\nFor Other Values (Log Scale): To find the baseline value (e.g., precision), exponentiate the intercept.\n\n\nFormula: \\[\n  \\text{Value} = e^{\\text{Estimate}}\n  \\]\n\n\nExample (phi_Intercept = 1.73): The baseline precision is \\(e^{1.73} \\approx 5.64\\).\n\n\n\n\n\nA coefficient shows how a predictor changes the outcome relative to the reference group. The key is to exponentiate the coefficient first.\n\n\nFor Odds (Logit Scale): The exponentiated coefficient is an Odds Ratio (OR).\n\n\nFormula: \\[\n  \\text{Odds Ratio (OR)} = e^{\\text{Coefficient}}\n  \\]\\[\n  \\text{Percentage Change in Odds} = (\\text{OR} - 1) \\times 100\\%\n  \\]\n\n\nExample (if_kill on mu = -1.22): The OR is \\(e^{-1.22} \\approx 0.295\\). The percentage change is \\((0.295 - 1) \\times 100\\% = -70.5\\%\\).\n\nInterpretation: The odds of having higher cover decrease by 70.5% when burro kills are present.\n\n\n\nFor Other Values (Log Scale): The exponentiated coefficient is a multiplicative factor.\n\n\nFormula: \\[\n  \\text{Multiplicative Factor} = e^{\\text{Coefficient}}\n  \\]\\[\n  \\text{Percentage Change} = (\\text{Factor} - 1) \\times 100\\%\n  \\]\n\n\nExample (if_kill on phi = -1.07): The factor is \\(e^{-1.07} \\approx 0.343\\). The percentage change is \\((0.343 - 1) \\times 100\\% = -65.7\\%\\).\n\nInterpretation: The presence of burro kills decreases precision by 65.7%.\n\n\n\n\n\n\n\n\n\n\n\nf0loo &lt;- loo::loo(fit0)\nf1loo &lt;- loo::loo(fit1)\n\nfc&lt;-loo::loo_compare(f0loo, f1loo)\nfc\n#      elpd_diff se_diff\n# fit1  0.0       0.0   \n# fit0 -2.7       1.6   \n\n\n\n\nburro_draws&lt;-fit1 %&gt;%\n  epred_draws(newdata = expand.grid(if_kill=unique(dat$if_kill)\n                                    ),re_formula = NA, dpar=TRUE) \n\n\n\n\n\n# Plot for location\nplot_mu &lt;- ggplot(burro_draws, aes(x = factor(if_kill), y = mu, fill = factor(if_kill))) +\n  geom_violin(alpha = 0.5, show.legend = FALSE) +\n  stat_pointinterval(show.legend = FALSE) +\n  labs(\n    title = \"Location\",\n    subtitle = \"Trampled Bare Ground\",\n    x = \"Cougar Kill Presence\",\n    y = \"Proportion \"\n  ) +\n  theme_classic()+scale_y_continuous(breaks = seq(0,1,0.2),limits = c(0, 1))\n\n# Plot for Zero-Inflation (zoi)\nplot_zoi &lt;- ggplot(burro_draws, aes(x = factor(if_kill), y = zoi, fill = factor(if_kill))) +\n  geom_violin(alpha = 0.5, show.legend = FALSE) +\n  stat_pointinterval(show.legend = FALSE) +\n  labs(\n    title = \"Zero-Inflation (zoi)\",\n     x = \"Cougar Kill Presence\",\n    y = \"Probability of Zero\"\n  ) +\n  theme_classic()+scale_y_continuous(breaks = seq(0,1,0.2),limits = c(0, 1))\n\n# Plot for One-Inflation (coi)\nplot_coi &lt;- ggplot(burro_draws, aes(x = factor(if_kill), y = coi, fill = factor(if_kill))) +\n  geom_violin(alpha = 0.5, show.legend = FALSE) +\n  stat_pointinterval(show.legend = FALSE) +\n  labs(\n    title = \"One-Inflation (coi)\",\n     x = \"Cougar Kill Presence\",\n    y = \"Probability of One\"\n  ) +\n  theme_classic() +scale_y_continuous(breaks = seq(0,1,0.2),limits = c(0, 1))\n\n# Plot for Precision (phi)\nplot_phi &lt;- ggplot(burro_draws, aes(x = factor(if_kill), y = phi, fill = factor(if_kill))) +\n  geom_violin(alpha = 0.5, show.legend = FALSE) +\n  stat_pointinterval(show.legend = FALSE) +\n  labs(\n    title = \"Scale\",\n    x = \"Cougar Kill Presence\",\n    y = \"Precision\"\n  ) +\n  theme_classic()+scale_y_continuous(breaks = seq(0,25,5),limits = c(0, 25))\n\n# --- 4. Combine Plots into a Single Figure ---\n# Arrange the four plots in a 2x2 grid\ncombined_plot &lt;- (plot_mu + plot_zoi) / (plot_coi + plot_phi) &\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n# Display the final combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Model Comparison Results\nOur analysis of the model comparison results shows that the location-scale model is the most supported by the data. This is indicated by its lowest LOO (Leave-One-Out Cross-Validation) information criterion value.\n\n6.3.3 Model Interpretation\nLocation (mean) part:\nOn average, the log-odds of the mean percentage of trampled bare ground are lower when burro kills are present compared to when they are absent (\\(\\beta_{[\\text{No-Yes}]}^{(l)}\\) = -1.22, 95% CI [-2.27, -0.18]). This corresponds to a 70.5% decrease in the odds of having a higher percentage of trampled ground, suggesting the presence of burro kills is associated with a lower average of trampled bare ground.\nScale (dispersion/phi) part: The log-precision (phi) is lower when burro kills are present, compared to when they are absent (\\(\\beta_{[\\text{No-Yes}]}^{(s)}\\) = -1.07, 95% CI [-2.01, -0.04]). This means the precision is multiplied by a factor of 0.34 (a 65.7% decrease), suggesting that the percentage of trampled bare ground exhibits more variation in areas where burro kills are present.\nZero-One Inflation (zoi) part: The log-odds of observing exactly 0% or 100% trampled bare ground are lower when burro kills are present (\\(\\beta_{[\\text{No-Yes}]}^{(l)}\\) = -1.71, 95% CI [-2.55, -0.89]). This represents an 81.9% decrease in the odds of observing completely untrampled ground, meaning areas with burro kills are less likely to be at this specific boundary.\nOne-inflation (coi) part: The log-odds of a boundary observation being 100% rather than 0% are lower when burro kills are present (\\(\\beta_{[\\text{No-Yes}]}^{(l)}\\) = -4.20, 95% CI [-12.07, -0.08]). This equates to a 98.5% decrease in the odds, implying that it’s extremely unlikely to find completely trampled ground (100%) where burro kills are present compared to where they are absent.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond Gaussian 2</span>"
    ]
  },
  {
    "objectID": "05_BeyondGaussian2.html#conclusion",
    "href": "05_BeyondGaussian2.html#conclusion",
    "title": "\n6  Beyond Gaussian 2\n",
    "section": "\n6.4 Conclusion",
    "text": "6.4 Conclusion\nDoes cougar predation affect the percentage of ground trampled by feral donkeys?\nAnswer: Yes, cougar predation, affects the average percentage of ground trampled by feral donkeys.\nDoes cougar predation affect the variability in the percentage of ground trampled by feral donkeys?\nAnswer: The model provides evidence that cougar predation affects the variability in the percentage of ground trampled by feral donkeys.",
    "crumbs": [
      "Main part",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond Gaussian 2</span>"
    ]
  },
  {
    "objectID": "06_ModelSelection.html",
    "href": "06_ModelSelection.html",
    "title": "\n7  Model Selection (introduction)\n",
    "section": "",
    "text": "8 Model selection\nThis section outlines general guidelines for model selection and refinement, based on the results from the previous examples (sections XX and Beyond Gaussian 1). These steps can be applied to any model fitting process, whether using glmmTMB, brms, or other packages.\n\n\nIdentify data type and plot raw data to understand the distribution and structure of the response variable. This helps in selecting an appropriate model family (e.g., Gaussian, Poisson, Negative Binomial, Beta).\n\nBegin with a simpler model (e.g., a location-only model assuming homoscedasticity) to establish a baseline for interpretation.\n\nCheck residual diagnostics to detect possible model misspecification, such as overdispersion or non-uniformity of residuals. If issues are detected, consider more complex models (e.g., location-scale models) that account for heterogeneity in variance.\n\nGradually increase model complexity: Introduce additional components, such as group-level effects on the scale (variance), and consider estimating correlations between random effects when theoretically or empirically justified—especially when supported by sufficient sample size.\n\nCompare models using information criteria:\n\n\nFor frequentist models: use AIC (Akaike Information Criterion)\nFor Bayesian models: use LOO (Leave-One-Out Cross-Validation) or WAIC (Widely Applicable Information Criterion).\n\nUltimately, the goal is not only to improve statistical fit but also to gain biological insights. For example, finding that a scale predictor improves model fit may suggest biologically meaningful heterogeneity.\nWe demonstrate these steps using the previous examples, where we started with a location-only model and then moved to a location-scale model to account for heterogeneity in variance. We also compared models using AICc and LOO criteria to select the best-fitting model. Although parts of the examples below may repeat content from earlier sections, we present them again here in sequence to illustrate a typical approach to model selection and refinement.\n\n\n\n\n\n\nOne important thing to keep in mind is that DHARMa does not work with models fitted using brms. This means that you cannot check residual diagnostics for the location-only double-hierarchical model (model 3) if it was fitted with brms. If you want to use DHARMa for residual diagnostics, you need to run model 2.5 instead - in other words, you cannot include the correlation part.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection (introduction)</span>"
    ]
  },
  {
    "objectID": "07_ModelSelection1.html",
    "href": "07_ModelSelection1.html",
    "title": "\n8  Model Selection (Example 1)\n",
    "section": "",
    "text": "In our ongoing exploration of Blue-footed Booby chick development, we previously investigated whether a chick’s scaled mass index (SMI) at fledgling was influenced by its hatching order (Model 2). Building upon that foundation, we now delve into the critical role of additional covariates, a common practice in ecological fieldwork where numerous factors are hypothesized to affect our response variable. While exhaustive inclusion of all potential covariates is often constrained by data availability, strategic model selection allows us to pinpoint those that provide significant explanatory power.\nOne particularly relevant covariate in this study is the chick’s hatching date. In this specific booby population, earlier hatching dates are frequently associated with enhanced chick survival. Conversely, later hatching can coincide with periods of increased food scarcity as ocean temperatures rise and fish prey disperse. Given these environmental pressures, we hypothesize that under the stressful conditions of late-season hatching, the disparity in fledgling mass between chicks might be more pronounced compared to those hatched earlier in the season. This suggests a potential interactive effect between hatching date and hatching order.\nIn the subsequent sections, we will systematically investigate this hypothesis by:\nIncorporating hatching date as a fixed effect into the location (mean) component of our model.\nAdding an interaction term between hatching date and hatching order to the location part of the model.\nConducting model comparisons to assess whether accounting for hatching date, both as a main effect and in interaction with hatching order, significantly improves our model’s fit and provides novel insights into chick fledgling success.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Selection (Example 1)</span>"
    ]
  },
  {
    "objectID": "07_ModelSelection1.html#figures",
    "href": "07_ModelSelection1.html#figures",
    "title": "\n8  Model Selection (Example 1)\n",
    "section": "\n8.1 Figures",
    "text": "8.1 Figures\n\nCodest_date_seq &lt;- seq(min(dat$ST_DATE), max(dat$ST_DATE), length.out = 50)\n\n\nemm_location &lt;- emmeans(model2_3b, ~ST_DATE*RANK,\n                        component = \"cond\", type = \"response\",\n                        at= list(ST_DATE = st_date_seq,RANK=unique(dat$RANK)))\n\ndf_location &lt;- as.data.frame(emm_location) %&gt;%\n  rename(\n    mean_log = emmean, \n    lwr = lower.CL,    \n    upr = upper.CL     \n  )\nemm_dispersion &lt;- emmeans(model2_3b, ~ RANK,\n                          component = \"disp\", type = \"response\")\ndf_dispersion &lt;- as.data.frame(emm_dispersion) %&gt;%\n  rename(\n    log_sigma = response, \n    lwr = lower.CL,     \n    upr = upper.CL     \n  )\npos &lt;- position_dodge(width = 0.4)\n\n\nplot_location_emmeans &lt;- ggplot(df_location, aes(x = ST_DATE, y = mean_log,color=factor(RANK),group=factor(RANK))) +\n   geom_ribbon(aes(ymin = lwr, ymax = upr, fill = factor(RANK)), alpha = 0.2, linetype = 0) +\n  geom_line(linewidth = 1)+\n  labs(\n    title = \"Location\",\n    x = \"Hatching date (days)\",\n    y = \"ln(SMI)\",\n    color=\"Hatching order\",\n    fill= \"Hatching order\"\n  ) +\n  theme_classic()+ \n    scale_y_continuous(breaks = seq(6.5,8,0.2),limits = c(6.5,8))+\n    scale_x_continuous(breaks= seq(0,160,30),limits = c(0, 160)) +theme(legend.position = 'bottom')\n\nplot_dispersion_emmeans &lt;- ggplot(df_dispersion, aes(x = RANK, y = log_sigma,color=factor(RANK))) +\n  geom_point(position = pos, size = 3) +\n  geom_errorbar(aes(ymin = lwr, ymax = upr), position = pos, width = 0.15) +\n  labs(\n    title = \"Scale\",\n    x = \"Hatching order\",\n    y = \"Standard Deviation\",\n    color=\"Hatching order\"\n  ) +\n  theme_classic() + scale_y_continuous(breaks = seq(0.07,0.12,0.01),limits = c(0.07, 0.12))+\n  theme(legend.position = \"none\")\n\nplot_location_emmeans + plot_dispersion_emmeans",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Selection (Example 1)</span>"
    ]
  },
  {
    "objectID": "08_ModelSelection2.html",
    "href": "08_ModelSelection2.html",
    "title": "\n9  Model Selection (Example 2)\n",
    "section": "",
    "text": "We revisit the dataset on Estrildid finches to examine how food supplementation affects gaze frequency toward dot patterns (as introduced in Beyond Gaussian 1 section).\nTo avoid repeating the structure of the previous example, we now take a more realistic modeling approach by incorporating phylogenetic non-independence as a random effect. The response variable, frequency, is a count variable, which supports the use of Poisson or Negative Binomial distributions. However, as visualised in the plots below, there is substantial between-species variation—both in mean gaze frequency and in the degree of within-species variability.\nTo better capture this structure, we include phylogeny as a random effect, distinct from the previously used species effect. While species was treated as a simple categorical grouping factor, the phylogenetic random effect accounts for shared evolutionary history and allows for correlated responses among closely related species.\nIn the following sections, we will:\n\nInclude phylogeny as a random effect in the location and/or scale parts of the model.\nCompare models with and without phylogenetic structure to evaluate whether accounting for evolutionary history improves model fit.\n\n\n\nStep 1\nSteps 2 and 3\nStep 4\nStep 5\n\n\n\n\n9.0.0.1 Identify data type and plot raw data\nThe violin plot by condition and sex shows that gaze frequency is generally higher in the food-deprived condition, with some individuals—especially females—showing very high values (over 500). The faceted line plot by species shows:\n\nConsistent directional trends (e.g., reduced gazing in the supplied condition)\nSpecies-specific differences in variance, suggesting possible heteroscedasticity\n\nThese patterns raise two important considerations:\n\nOverdispersion is likely present, particularly in the deprived condition, where the spread of counts is large.\nA phylogenetic effect may be at play, since species vary not only in their average gaze frequencies but also in how consistent individuals are within each species - possibly due to traits shaped by common ancestry.\n\n\ndat_pref &lt;- read.csv(here(\"data\", \"AM_preference.csv\"), header = TRUE)\n\ndat_pref &lt;- dat_pref %&gt;%\n  dplyr::select(-stripe, -full, -subset) %&gt;%\n  rename(frequency = dot) %&gt;%\n  mutate(species = phylo, across(c(condition, sex), as.factor))\n  \nggplot(dat_pref, aes(x = condition,\n                     y = frequency)) +\n    geom_violin(color = \"#8B8B83\", fill = \"white\",\n              width = 1.2, alpha = 0.3) +\n  geom_jitter(aes(shape = sex, color = sex),\n              size = 3, alpha = 0.8,\n              width = 0.15, height = 0) +\n  labs(\n    title = \"Total frequency of gazes towards dot patterns\",\n    x = \"Condition\",\n    y = \"Total frequency of gazes (1hr)\"\n  ) +\n  scale_shape_manual(values = c(\"M\" = 17, \"F\" = 16),\n                     labels = c(\"M\" = \"Male\", \"F\" = \"Female\")) +\n  scale_color_manual(values = c(\"M\" = \"#009ACD\", \"F\" = \"#FF4D4D\"),\n                     labels = c(\"M\" = \"Male\", \"F\" = \"Female\")) +\n  theme_classic(base_size = 16) +\n  theme(\n    axis.text = element_text(color = \"#6E7B8B\", size = 14),\n    axis.title = element_text(color = \"#6E7B8B\", size = 14),\n    legend.title = element_text(color = \"#6E7B8B\"),\n    legend.text = element_text(color = \"#6E7B8B\"),\n    legend.position = \"right\",\n  axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\nggplot(dat_pref, aes(x = dat_pref$condition, y = dat_pref$frequency)) +\n  geom_point(alpha = 0.5) +\n  labs(\n    title = \"Total frequency of gazes towards dot patterns by species\",\n    x = \"Condition\",\n    y = \"Total frequency of gazes (1hr)\"\n  ) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = species, color = species)) +\n  theme_classic() +\n  facet_wrap(~ species)\n\n\n\n\n\n\n\n\n\n\n\n9.0.0.2 Begin with a simpler model and check residual diagnostics\nFrom the previous analysis, we already know that a location-only model without phylogenetic effects does not adequately capture the structure of the data. However, here we re-examine this model before moving on.\n\nmodel_0 &lt;- glmmTMB(\n  frequency ~ 1 +  condition + sex + (1 | species) + (1 | id), # location part (mean)\n  data = dat_pref,\n  family = nbinom2(link = \"log\")\n)\n\nTo evaluate model adequacy, we use the DHARMa package to simulate and visualise residuals. The plots allow us to assess residual uniformity, potential over- or underdispersion, outliers, and leverage.\n\n# main diagnostic plots\n\nmodel0_res &lt;- simulateResiduals(model_0, plot = TRUE, seed = 42)\n\n\n\n\n\n\n# formal test for over/underdispersion\ntestDispersion(model0_res) \n\n\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.45606, p-value = 0.456\nalternative hypothesis: two.sided\n\n\nAlthough the dispersion test (p = 0.456) suggests no clear evidence of overdispersion based on the mean–variance relationship, the KS test indicates that the residuals deviate from a uniform distribution. These two tests assess different aspects of model adequacy: the dispersion test evaluates whether the variance is correctly specified as a function of the mean, while the KS test can detect broader misfits, such as unmodeled structure, zero inflation, or group-specific heteroscedasticity. Thus, even in the absence of overdispersion, the presence of non-uniform residuals supports the need for a more flexible model.\nSince we have found the limitations of the model without phylogenetic effects, we now proceed to fit the same model using the brms package, this time including the phylogenetic effect as a random effect to account for the non-independence among species. Including both species and phylogeny as random effects allows us to distinguish between two sources of variation: the phylogenetic effect captures correlations arising from shared evolutionary history, whereas the species-level random intercept absorbs residual species-specific variation not accounted for by the tree—such as ecological or methodological factors. Including both terms helps avoid underestimating heterogeneity at the species level and provides a more accurate partitioning of variance.\nFirst, we run the same model as above, but using the brms package. Then, we will gradually increase the model complexity by adding phylogenetic effects and a scale part to the model.\n\nformula_eg2.0 &lt;- bf(\n  frequency ~ 1 + condition + sex + (1 | species) + (1 | id)\n)\n\nprior_eg2.0 &lt;- default_prior(formula_eg2.0, \n                        data = dat_pref, \n                        family = negbinomial(link = \"log\")\n)\n\nmodel_eg2.0 &lt;- brm(formula_eg2.0, \n            data = dat_pref, \n            prior = prior_eg2.0,\n            chains = 2, \n            iter = 5000, \n            warmup = 3000,\n            thin = 1,\n            family = negbinomial(link = \"log\"),\n            save_pars = save_pars(all = TRUE)\n            # control = list(adapt_delta = 0.95)\n)\n\nThen, we add the phylogenetic effect as a random effect to the model. This allows us to account for the non-independence among species due to shared evolutionary history.\n\ntree &lt;- read.nexus(here(\"data\", \"AM_est_tree.txt\"))\ntree　&lt;-force.ultrametric(tree, method = \"extend\")\n# is.ultrametric(tree)\n\ntip &lt;- c(\"Lonchura_cantans\",\"Uraeginthus_bengalus\",\"Neochmia_modesta\",\n         \"Lonchura_atricapilla\",\"Padda_oryzivora\",\"Taeniopygia_bichenovii\",\n         \"Emblema_pictum\",\"Neochmia_ruficauda\",\"Lonchura_maja\",\n         \"Taeniopygia_guttata\",\"Chloebia_gouldiae\",\"Lonchura_castaneothorax\")\ntree &lt;- KeepTip(tree, tip, preorder = TRUE, check = TRUE)\n\n# Create phylogenetic correlation matrix\nA &lt;- ape::vcv.phylo(tree, corr = TRUE)\n\n# Specify the model formula (location-only)\nformula_eg2.1 &lt;- bf(\n  frequency ~ 1 + condition + sex + \n    (1 | a | gr(phylo, cov = A)) +  # phylogenetic random effect\n    (1 | species) +             # non-phylogenetic species-level random effect (ecological factors)\n    (1 | id)                    # individual-level random effect\n)\n\nprior_eg2.1 &lt;- brms::get_prior(\n  formula = formula_eg2.1, \n  data = dat_pref, \n  data2 = list(A = A),\n  family = negbinomial(link = \"log\")\n)\n\nmodel_eg2.1 &lt;- brm(\n  formula = formula_eg2.1, \n  data = dat_pref, \n  data2 = list(A = A),\n  chains = 2, \n  iter = 12000, \n  warmup = 10000,\n  thin = 1,\n  family = negbinomial(link = \"log\"),\n  prior = prior_eg2.1,\n  control = list(adapt_delta = 0.95),\n  save_pars = save_pars(all = TRUE)\n)\n\n\n\n\n\n9.0.0.3 Gradually increase model complexity\nIn the previous model, variation in individual gaze frequency was modeled assuming a constant dispersion parameter across all groups. However, visual inspection of the raw data suggests heteroscedasticity (i.e., group-specific variance). To accommodate this, we now fit a location-scale model where the shape (dispersion) parameter is allowed to vary by condition and sex.\nThis model helps capture structured variability in the data and may reveal whether the precision (or variability) of gaze frequency differs systematically between experimental groups.\n\n# Specify the model formula (location-scale) - the scale part does not include random effects\nformula_eg2.2 &lt;- bf(\n  frequency ~ 1 + condition + sex + \n    (1 | a | gr(phylo, cov = A)) +  \n    (1 | species) +          \n    (1 | id),              \nshape ~ 1 + condition + sex\n)\n\nprior_eg2.2 &lt;- brms::get_prior(\n  formula = formula_eg2.2, \n  data = dat_pref, \n  data2 = list(A = A),\n  family = negbinomial(link = \"log\", link_shape = \"log\")\n)\n\nmodel_eg2.2 &lt;- brm(\n  formula = formula_eg2.2, \n  data = dat_pref, \n  data2 = list(A = A),\n  chains = 2, \n  iter = 12000, \n  warmup = 10000,\n  thin = 1,\n  family = negbinomial(link = \"log\", link_shape = \"log\"),\n  prior = prior_eg2.2,\n  control = list(adapt_delta = 0.95),\n  save_pars = save_pars(all = TRUE)\n)\n\n\n# specify the model formula (location-scale)\n## the scale part includes phylogenetic random effects and individual-level random effects\n\nformula_eg2.3 &lt;- bf(\n  frequency ~ 1 + condition + sex + \n    (1 |a| gr(phylo, cov = A)) +  # phylogenetic random effect\n    (1 | species) +            # non-phylogenetic species-level random effect (ecological factors)\n    (1 | id),              # individual-level random effect\nshape ~ 1 + condition + sex +\n    (1 |a| gr(phylo, cov = A)) +  \n    (1 | species) +  \n    (1 | id)          \n    )\n\nprior_eg2.3 &lt;- brms::get_prior(\n  formula = formula_eg2.3, \n  data = dat_pref, \n  data2 = list(A = A),\n  family = negbinomial(link = \"log\", link_shape = \"log\")\n)\n\nmodel_eg2.3 &lt;- brm(\n  formula = formula_eg2.3, \n  data = dat_pref, \n  data2 = list(A = A),\n  chains = 2, \n  iter = 12000, \n  warmup = 10000,\n  thin = 1,\n  family = negbinomial(link = \"log\", link_shape = \"log\"),\n  prior = prior_eg2.3,\n  control = list(adapt_delta = 0.95),\n  save_pars = save_pars(all = TRUE)\n)\n\nsummary(model_eg2.3)\n\n\n\n\n\n9.0.0.4 Compare models using information criteria\nTo assess whether model fit improves with increased complexity, we compare models using Leave-One-Out Cross-Validation (LOO).\n\noptions(future.globals.maxSize = 2 * 1024^3)\nloo_eg2.0 &lt;- loo::loo(model_eg2.0, moment_match = TRUE,\n                      reloo = TRUE, cores = 2)\nloo_eg2.1 &lt;- loo::loo(model_eg2.1, moment_match = TRUE, \n                      reloo = TRUE, cores = 2)\nloo_eg2.2 &lt;- loo::loo(model_eg2.2, moment_match = TRUE,\n                      reloo = TRUE, cores = 2)\nloo_eg2.3 &lt;- loo::loo(model_eg2.3, moment_match = TRUE,\n                      reloo = TRUE, cores = 2)\n\nfc_eg2 &lt;- loo::loo_compare(loo_eg2.0, loo_eg2.1, loo_eg2.2, loo_eg2.3)\n\nprint(fc_eg2)\n#             elpd_diff se_diff\n# model_eg2.3   0.0       0.0  \n# model_eg2.1 -11.5       4.5  \n# model_eg2.2 -11.7       5.3  \n# model_eg2.0 -13.5       4.9  \n\nsummary(model_eg2.3)\n\nModel eg2.3, which includes both phylogenetic and species-level effects on the location and scale parts, performs best (highest elpd).\nELPD stands for Expected Log Predictive Density. It is a measure of out-of-sample predictive accuracy—that is, how well a model is expected to predict new, unseen data. - It is used in Bayesian model comparison, especially when using LOO-CV (Leave-One-Out Cross-Validation). - Higher ELPD means better predictive performance. - It is calculated by taking the log-likelihood of each observation, averaged over posterior draws, and then summed over all observations. - It rewards models that fit the data well without over-fitting.\n\n\n Family: negbinomial \n  Links: mu = log; shape = log \nFormula: frequency ~ condition + sex + (1 | a | gr(phylo, cov = A)) + (1 | species) + (1 | id) \n         shape ~ condition + sex + (1 | a | gr(phylo, cov = A)) + (1 | species) + (1 | id)\n   Data: dat_pref (Number of observations: 190) \n  Draws: 2 chains, each with iter = 12000; warmup = 10000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 95) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.31      0.11     0.05     0.51 1.00      533      501\nsd(shape_Intercept)     0.21      0.15     0.01     0.57 1.00     1125     1444\n\n~phylo (Number of levels: 12) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(Intercept)                      0.49      0.25     0.05     1.04 1.00\nsd(shape_Intercept)                0.74      0.38     0.09     1.63 1.00\ncor(Intercept,shape_Intercept)     0.51      0.44    -0.64     0.99 1.00\n                               Bulk_ESS Tail_ESS\nsd(Intercept)                       759      670\nsd(shape_Intercept)                1060      983\ncor(Intercept,shape_Intercept)      854     1112\n\n~species (Number of levels: 12) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.27      0.20     0.01     0.74 1.00      833     1644\nsd(shape_Intercept)     0.46      0.33     0.02     1.24 1.00      826     1519\n\nRegression Coefficients:\n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                   5.07      0.30     4.46     5.65 1.00     1501\nshape_Intercept             0.85      0.50    -0.16     1.84 1.00     1384\nconditionsupplied          -0.77      0.11    -0.99    -0.54 1.00     3387\nsexM                       -0.15      0.13    -0.40     0.11 1.00     2191\nshape_conditionsupplied    -0.70      0.26    -1.22    -0.21 1.00     2251\nshape_sexM                  0.12      0.26    -0.40     0.62 1.00     2814\n                        Tail_ESS\nIntercept                   2213\nshape_Intercept             1884\nconditionsupplied           2669\nsexM                        2480\nshape_conditionsupplied     2726\nshape_sexM                  2867\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\nAccounting for phylogeny captures much of the between-species structure, it highlights evolutionary constraints or shared ecological traits. The remaining species-specific variance suggests additional factors (e.g., experimental nuances) not captured by the tree alone.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Model Selection (Example 2)</span>"
    ]
  },
  {
    "objectID": "09_Simulation.html",
    "href": "09_Simulation.html",
    "title": "\n10  Simulation\n",
    "section": "",
    "text": "Here, we examine how sample size (N) affects estimates of the sample mean and sample standard deviation (SD). For each N = 2 to 50, draw 10000 samples from the standard normal distribution \\(\\mathcal{N}(0, 1)\\). Then, compute mean and SD for each sample. After that, calculate the average of these estimates to see how close they are to the true values (mean = 0, SD = 1).\n\nset.seed(2025)\n\nn_reps &lt;- 10000\nNs     &lt;- 2:50\n\n# generate all combinations of sample size (N) and replicate number\nresults &lt;- expand.grid(\n  N      = Ns,\n  replic = seq_len(n_reps)\n) %&gt;%\n  arrange(N, replic)\n\n# pre-calculate how many total simulations to run and \n# generate a vector of sample sizes for all rows\nn_total &lt;- nrow(results)\nNs_vec &lt;- results$N\n\n# system.time({\n# generate all random samples in one go (total number of draws = sum of all Ns)\nsamples_all &lt;- rnorm(sum(Ns_vec), mean = 0, sd = 1)\n\n# assign an ID to each sample, to keep track of which row (i.e., which N) it belongs to\nrow_id &lt;- rep(seq_along(Ns_vec), times = Ns_vec)\n\n# split the generated samples by row\nsplit_samples &lt;- split(samples_all, row_id)\n\n# compute sample mean and SD for each group\nmeans &lt;- sapply(split_samples, mean)\nsds   &lt;- sapply(split_samples, sd)\n# })\n\n# add results into the data frame\nresults$samp_mean &lt;- means\nresults$samp_sd &lt;- sds\n\n# summarise the average sample mean and SD per sample size (N)\nsummary2 &lt;- results %&gt;%\n  group_by(N) %&gt;%\n  summarize(\n    avg_mean = mean(samp_mean),\n    avg_sd = mean(samp_sd),\n    .groups = \"drop\"\n  )\n\n# plot A: Average Sample Mean vs. N\np_avg_mean &lt;- ggplot(summary2, aes(x = N, y = avg_mean)) +\n  geom_point(color = \"#1f77b4\", size = 2) +\n  geom_line(color = \"#1f77b4\", size = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Average Sample Mean vs. Sample Size (N = 2…50)\",\n    subtitle = \"True mean = 0 (dashed line)\",\n    x = \"Sample size (N)\",\n    y = \"Average of 10000 sample means\"\n  ) +\n  coord_cartesian(ylim = c(-0.4, 0.4)) +\n  theme_classic(base_size = 13) +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\n# plot B: Average Sample SD vs. N\np_avg_sd &lt;- ggplot(summary2, aes(x = N, y = avg_sd)) +\n  geom_point(color = \"#ff7f0e\", size = 2) +\n  geom_line(color = \"#ff7f0e\", size = 1) +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Average Sample SD vs. Sample Size (N = 2…50)\",\n    subtitle = \"True SD = 1 (dashed line)\",\n    x = \"Sample size (N)\",\n    y = \"Average of 10000 sample SDs\"\n  ) +\n  # coord_cartesian(ylim = c(-1.0, 1.0)) +\n  theme_classic(base_size = 13) +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\n# display the two plots\np_avg_mean\n\n\n\n\n\n\np_avg_sd\n\n\n\n\n\n\n\nFor small sample sizes, sample variance and standard deviation tend to be biased, as shown in the bottom plot, the average sample SD is clearly lower than 1 when N is small. Once N exceeds approximately 20, the average sample SD (and variance) gets much closer to 1, and the bias becomes negligible.\n\n11 References\n\nCleasby IR, Burke T, Schroeder J, Nakagawa S. (2011) Food supplements increase adult tarsus length, but not growth rate, in an island population of house sparrows (Passer domesticus). BMC Research Notes. 4:1-1. doi: 10.1186/1756-0500-4-431\nDrummond H, Rodriguez C, Ortega S. (2025). Long-Term Insights into Who Benefits from Brood Reduction. Behavioral Ecology. doi: 10.1093/beheco/araf050\nMizuno A, Soma M. (2023) Pre-existing visual preference for white dot patterns in estrildid finches: a comparative study of a multi-species experiment. Royal Society Open Science. 10:231057. doi: 10.1098/rsos.231057\nLundgren EJ, Ramp D, Middleton OS, Wooster EI, Kusch E, Balisi M, Ripple WJ, Hasselerharm CD, Sanchez JN, Mills M, Wallach AD. (2022) A novel trophic cascade between cougars and feral donkeys shapes desert wetlands. Journal of Animal Ecology. 91:2348-57. doi: 10.1111/1365-2656.13766\nAki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, Paul-Christian Bürkner (2021). Rank-Normalization, Folding, and Localization: An Improved Rhat for Assessing Convergence of MCMC (with Discussion). Bayesian Analysis. 16:667-718. doi: 10.1214/20-BA1221\n\n12 Information about R session\nThis section shows the current R session information, including R version, platform, and loaded packages.\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Edmonton\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] knitr_1.50          kableExtra_1.4.0    here_1.0.1         \n [4] gt_1.0.0            tidybayes_3.0.7     patchwork_1.3.1    \n [7] bayesplot_1.13.0    MuMIn_1.48.11       loo_2.8.0          \n[10] DHARMa_0.4.7        TreeTools_1.14.0    rstan_2.32.7       \n[13] StanHeaders_2.32.10 phytools_2.4-4      maps_3.4.3         \n[16] glmmTMB_1.1.11      emmeans_1.11.1      cmdstanr_0.9.0.9000\n[19] brms_2.23.0         Rcpp_1.1.0          arm_1.14-4         \n[22] lme4_1.1-37         Matrix_1.7-3        MASS_7.3-65        \n[25] ape_5.8-1           broom.mixed_0.2.9.6 broom_1.0.8        \n[28] lubridate_1.9.4     forcats_1.0.0       stringr_1.5.1      \n[31] purrr_1.0.4         readr_2.1.5         tidyr_1.3.1        \n[34] ggplot2_3.5.2       tidyverse_2.0.0     tibble_3.3.0       \n[37] dplyr_1.1.4        \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3      tensorA_0.36.2.1        rstudioapi_0.17.1      \n  [4] jsonlite_2.0.0          magrittr_2.0.3          TH.data_1.1-3          \n  [7] estimability_1.5.1      farver_2.1.2            nloptr_2.2.1           \n [10] rmarkdown_2.29          vctrs_0.6.5             minqa_1.2.8            \n [13] RCurl_1.98-1.17         htmltools_0.5.8.1       distributional_0.5.0   \n [16] curl_6.4.0              DEoptim_2.2-8           parallelly_1.45.0      \n [19] htmlwidgets_1.6.4       sandwich_3.1-1          zoo_1.8-14             \n [22] TMB_1.9.17              igraph_2.1.4            lifecycle_1.0.4        \n [25] iterators_1.0.14        pkgconfig_2.0.3         R6_2.6.1               \n [28] fastmap_1.2.0           rbibutils_2.3           future_1.58.0          \n [31] digest_0.6.37           numDeriv_2016.8-1.1     colorspace_2.1-1       \n [34] furrr_0.3.1             ps_1.9.1                rprojroot_2.0.4        \n [37] textshaping_1.0.1       labeling_0.4.3          clusterGeneration_1.3.8\n [40] timechange_0.3.0        abind_1.4-8             mgcv_1.9-3             \n [43] compiler_4.4.2          bit64_4.6.0-1           withr_3.0.2            \n [46] doParallel_1.0.17       backports_1.5.0         inline_0.3.21          \n [49] optimParallel_1.0-2     QuickJSR_1.8.0          pkgbuild_1.4.8         \n [52] R.utils_2.13.0          scatterplot3d_0.3-44    tools_4.4.2            \n [55] R.oo_1.27.1             glue_1.8.0              quadprog_1.5-8         \n [58] nlme_3.1-168            R.cache_0.17.0          grid_4.4.2             \n [61] checkmate_2.3.2         PlotTools_0.3.1         generics_0.1.4         \n [64] gtable_0.3.6            tzdb_0.5.0              R.methodsS3_1.8.2      \n [67] hms_1.1.3               xml2_1.3.8              ggdist_3.3.3           \n [70] foreach_1.5.2           pillar_1.11.0           posterior_1.6.1        \n [73] splines_4.4.2           lattice_0.22-7          bit_4.6.0              \n [76] survival_3.8-3          tidyselect_1.2.1        arrayhelpers_1.1-0     \n [79] reformulas_0.4.1        gridExtra_2.3           V8_6.0.4               \n [82] svglite_2.2.1           stats4_4.4.2            xfun_0.52              \n [85] expm_1.0-0              bridgesampling_1.1-2    matrixStats_1.5.0      \n [88] stringi_1.8.7           yaml_2.3.10             pacman_0.5.1           \n [91] boot_1.3-31             evaluate_1.0.4          codetools_0.2-20       \n [94] cli_3.6.5               RcppParallel_5.1.10     systemfonts_1.2.3      \n [97] xtable_1.8-4            Rdpack_2.6.4            processx_3.8.6         \n[100] globals_0.18.0          coda_0.19-4.1           svUnit_1.0.6           \n[103] rstantools_2.4.0        bitops_1.0-9            Brobdingnag_1.2-9      \n[106] listenv_0.9.1           phangorn_2.12.1         viridisLite_0.4.2      \n[109] mvtnorm_1.3-3           scales_1.4.0            combinat_0.0-8         \n[112] rlang_1.1.6             fastmatch_1.1-6         multcomp_1.4-28        \n[115] mnormt_2.1.1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  }
]